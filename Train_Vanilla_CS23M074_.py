# -*- coding: utf-8 -*-
"""vanillatrain-py.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W8SGtH1AOX0iYyHTE7jNckUQmX9pQq97

<a href="https://colab.research.google.com/github/Shreyash007/CS6910-Assignment3/blob/main/Assignment3.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

import pandas as pd
import argparse
import numpy as np
import random
from typing import Tuple
import torch
import torch.nn as nn
import torch.optim as optim
import string
import random
from collections import Counter
import torch
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader
SEED = random.randint(1, 1000)
random.seed(SEED)
torch.manual_seed(SEED)
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch import Tensor
import pandas as pd
import wandb
import torch
unknown_token=64

BATCH_SIZE = 512
N_EPOCHS = 1
CLIP = 1
emb_dim = 256
hidden_layer_dim = 256
attn_dim = 128
dropout = 0.6
opti = "adam"

# import pandas as pd
# from torch.nn.utils.rnn import pad_sequence
# from torch.utils.data import DataLoader

# import math
# import time
# import pandas as pd
# import numpy as np
# import random
# from typing import Tuple
# import torch
# import torch.nn as nn
# import torch.optim as optim
# import string
# import random
# from collections import Counter
# import torch
# from torch.nn.utils.rnn import pad_sequence
# from torch.utils.data import DataLoader

# ///////////////////////////////////////////////////////////
# if __name__ == "__main__":
# Initialize argument parser
parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
# Define arguments for hyperparameters
parser.add_argument("-wp", '--wandb_project', help='Project name used to track experiments in Weights & Biases dashboard', type=str, default='Deep_Learning_Seq2Seq_Transliteration_Assignment_3  ')
parser.add_argument("-we", "--wandb_entity", type=str, help="Wandb Entity used to track experiments in the Weights & Biases dashboard.", default="cs23m074")
parser.add_argument('-ep', '--epochs', help="Number of epochs to train neural network.", type=int, default=10)
parser.add_argument("-dp", "--dropout", default=0.2, type=float, choices=[0, 0.2,0.3 , 0.4 , 0.6])
parser.add_argument("-lg", "--logger", type=bool, default=False, choices=[True, False], help="Log to wandb or not")

parser.add_argument('-tpd', '--test_path_directory', help="Dataset", type=str, default='/content/drive/MyDrive/DL/A3_Data/aksharantar_sampled/mar/mar_test.csv')
parser.add_argument('-trpd', '--train_path_directory', help="Dataset", type=str, default='/content/drive/MyDrive/DL/A3_Data/aksharantar_sampled/mar/mar_train.csv')
parser.add_argument('-vpd', '--val_path_directory', help="Dataset", type=str, default='/content/drive/MyDrive/DL/A3_Data/aksharantar_sampled/mar/mar_valid.csv')

parser.add_argument('-cp', '--clip', help='choices: [1,2,3]', choices=[1,2,3], type=int, default=1)
parser.add_argument("-hls", "--hidden_layer_size", default=256, type=int, choices=[64,128 , 256,512])
parser.add_argument("-es", "--embedding_size", type=int, default=256 , choices=[64,128,256])


# Parse arguments from command line
args = parser.parse_args()
file_paths = [
    args.train_path_directory,
    args.val_path_directory,
    args.test_path_directory
]

# /////////////////////////////////////////////////////////////////

"""# LOADING DATA FROM CSV FILES"""

def load_data(path):
    data = pd.read_csv(path, header=None, names=["en", "ma", ""], skip_blank_lines=True)
    data = data.dropna(subset=['en', 'ma'])
    return data[['en', 'ma']]

BATCH_SIZE = 512
N_EPOCHS = 1
CLIP = 1
emb_dim = 256
hidden_layer_dim = 256
attn_dim = 128
dropout = 0.6
opti = "adam"

# Load data for train, val, and test
train, val, test = [load_data(path) for path in file_paths]

# Display test data and lengths
# print(test)
test_en = test['en'].tolist()
test_ma = test['ma'].tolist()
# print(test_en)
# print(test_ma)
print(len(train))
print(len(test))
print(len(val))

"""#TOKENIZE DATASET"""

def unique_tokenize(data):
    english_tokens = set(''.join(data['en']))
    marathi_tokens = set(''.join(data['ma']))
    return sorted(list(marathi_tokens)), sorted(list(english_tokens))
marathi_tokens , english_tokens = unique_tokenize(train)
print(english_tokens)
print(len(marathi_tokens))

"""#TOKEN MAP"""

#REWRITE AGAIN
def tokenize_map(hindi_tokens , english_tokens):
    english_token_map = dict([(ch,i+1) for i,ch in enumerate(english_tokens)])
    marathi_token_map = dict([(ch,i+1) for i,ch in enumerate(marathi_tokens)])
    reverse_marathi_token_map = dict([(i+1,ch) for i,ch in enumerate(marathi_tokens)])
    #Adding blank space

    marathi_token_map[" "] = 0
    english_token_map[" "] = 0
    #addin BOS and EOS token
    marathi_token_map[';']=65
    marathi_token_map['.']=66
    english_token_map[';']=27
    english_token_map['.']=28
    marathi_token_map['<unk>']=64

    reverse_marathi_token_map[64]='<unk>'
    reverse_marathi_token_map[65]=';'
    reverse_marathi_token_map[66]='.'
    reverse_marathi_token_map[0]=''

    return marathi_token_map, english_token_map,reverse_marathi_token_map

mar_token_map, eng_token_map,reverse_marathi_token_map = tokenize_map(marathi_tokens , english_tokens)
print(mar_token_map)
print(reverse_marathi_token_map)
print(eng_token_map)

print('mar:',len(mar_token_map))
print('eng:',len(eng_token_map))

"""#MAXIMUM WORD LENGTH THAT ARE PRESENT IN THE DATASET



"""

x = test['en'].values
y = test['ma'].values

#Getting max length
max_eng_len = 2+max([len(i) for i in x])
max_mar_len = 2+max([len(i) for i in y])
print(max_eng_len,max_mar_len)

"""# ONE HOT ENCODING/EMBEDDING OUR INPUT


"""

import torch
unknown_token=64
def process(data):
    x, y = data['en'].values, data['ma'].values

    # Add start and end tokens to sentences
    x = [';' + sentence + '.' for sentence in x]
    y = [';' + sentence + '.' for sentence in y]
    a = torch.zeros((len(x), max_eng_len), dtype=torch.int64)
    b = torch.zeros((len(y), max_eng_len), dtype=torch.int64)

    # Process data
    for i, (xx, yy) in enumerate(zip(x, y)):
        for j, ch in enumerate(xx):
            a[i, j] = eng_token_map.get(ch, unknown_token)

        for j, ch in enumerate(yy):
            b[i, j] = mar_token_map.get(ch, unknown_token)


    # Create list of tuples (input tensor, target tensor)
    data = [(a[i], b[i]) for i in range(len(x))]

    return data


train_process=process(train)
val_process=process(val)
test_process=process(test)#print(train_process.shape)
print('\n')
print('num of rows:',len(train_process))
print('num of columns:',len(train_process[0]))
print(train_process[0][1])

def reverse_tokenize(data):
    data = list(map(int, data))
    predicted_seq = ''.join(map(lambda idx: reverse_marathi_token_map[idx], data))
    return predicted_seq

"""#DATA LOADER"""

BATCH_SIZE = 16


def dataL(P, BATCH_SIZE, shuffle_bool):
    iter = DataLoader(P, batch_size=BATCH_SIZE, shuffle=shuffle_bool)

    return iter



train_iter = dataL(train_process, BATCH_SIZE, False)
valid_iter = dataL(val_process, BATCH_SIZE, False)
test_iter = dataL(test_process, BATCH_SIZE, False)

print(len(train_iter))
print(len(test_iter))
print(len(valid_iter))

# !wandb login f6b40fc0bcc13c1d6117718b14ef1ddd1c68a700 #my API key for wandb login
# import wandb

"""#VANILLA SEQ2SEQ MODEL ON TEST DATASET



"""

# Define the vocabulary of English and Devanagari characters

# Define the maximum sequence lengths for input and output sequences
MAX_LEN_EN = 30
MAX_LEN_MA = 30

# Define the start and end of sequence tokens
BOS_token = 28
EOS_token = 29


class Encoder(nn.Module):
    def __init__(self, input_size, embedding_size, hidden_size, num_layers=1 , cell_type="lstm", p=0.5):
        super().__init__()
        eL = [input_size, embedding_size, hidden_size, num_layers , cell_type, p]
        self.embedding = nn.Embedding(eL[0],eL[1])
        self.hidden_size = eL[2]
        self.num_layers = eL[3]
        self.cell_type = eL[4]
        self.dropout=nn.Dropout(eL[5])
        prob = p
        if cell_type.upper() == "GRU":
            self.rnn = nn.GRU(input_size, hidden_size, num_layers,dropout=prob)
        if cell_type.upper() == "LSTM":
            self.rnn = nn.LSTM(input_size, hidden_size, num_layers,dropout=prob)
        if cell_type.upper() != "RNN":
            self.rnn = nn.GRU(input_size, hidden_size, num_layers,dropout=prob)
        else:
            self.rnn = nn.RNN(input_size, hidden_size, num_layers,dropout=prob)
    def forward(self, x):

        x = x.permute(1,0)
        pre_embedding = self.embedding(x)
        embedding=self.dropout(pre_embedding)
        if self.cell_type != "lstm":
            hidden, cell = self.rnn(embedding)
        elif self.cell_type == "lstm":
            outputs,(hidden,cell)=self.rnn(embedding)
        return hidden, cell


class Decoder(nn.Module):
    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers=1, cell_type="lstm",p=0.5):
        super().__init__()
        dL = [input_size, embedding_size, hidden_size, output_size, num_layers, cell_type,p]
        self.embedding=nn.Embedding(dL[0], dL[1])
        self.hidden_size = dL[2]
        self.cell_type=dL[5]
        self.num_layers = dL[4]
        self.dropout=nn.Dropout(dL[6])

        self.output_size = dL[3]
        if cell_type.upper() == "GRU":
            self.rnn = nn.GRU(output_size, hidden_size, num_layers)
        if cell_type.lower() != "lstm":
            self.rnn = nn.RNN(output_size, hidden_size, num_layers)
        if cell_type.upper() == "RNN":
            self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers,dropout=p)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x, hidden,cell):

       x = x.unsqueeze(0)
       pre_embeddingD = self.embedding(x)
       embedding= self.dropout(pre_embeddingD)

       if self.cell_type!="lstm":
         outputs, hidden = self.rnn(embedding, hidden)
       else:
         outputs,(hidden,cell) = self.rnn(embedding, (hidden,cell))


       predictions=self.fc(outputs)

       tempD = 0
       predictions = predictions.squeeze(tempD)

       if self.cell_type != "lstm":
         return predictions, hidden
       else:
         return predictions, hidden, cell

class Seq2Seq(nn.Module):
    def __init__(self,encoder,decoder,cell_type="lstm"):
        sL = [encoder,decoder,cell_type]
        super().__init__()
        self.encoder = sL[0]
        self.decoder = sL[1]
        self.cell_type = sL[2]

    def forward(self, source, target, teacher_force_ratio=0.5):
        sFL = [source, target, teacher_force_ratio]
        batch_size = sFL[0].shape[0]

        target_len = sFL[1].shape[1]
        target_vocab_size = self.decoder.output_size

        outputs = torch.zeros(target_len,batch_size,target_vocab_size).to(device)

        cell, hidden = self.encoder(sFL[0])

        target = sFL[1].permute(1,0)
        x = target[0,:]
        t = 1
        while t < target_len:
          if self.cell_type!="lstm":
            output, hidden = self.decoder(x,hidden,cell)
          else:
           output,hidden,cell =self.decoder(x,hidden,cell)

          outputs[t] = output

          best_guess = output.argmax(1)
          if random.random() >= teacher_force_ratio:
            x = best_guess
          else:
            x = target[t]
          t += 1
        return outputs

device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
num_epochs=10
learning_rate=0.001
len_token_map = [len(eng_token_map), len(mar_token_map)]
input_size_encoder = len_token_map[0]
input_size_decoder= len_token_map[1]
output_size= len_token_map[1]
max_size_E_D = [29, 67]
encoder_embedding_size = max_size_E_D[0]
decoder_embedding_size = max_size_E_D[1]

cell_type="gru"
hidden_size=256
num_layers=4
enc_dropout=0.2
dec_dropout=0.2
train_L = [input_size_encoder, encoder_embedding_size, hidden_size, num_layers , cell_type]
encoder_net=Encoder(len_token_map[0],max_size_E_D[0],train_L[2],train_L[3],train_L[4],p=enc_dropout).to(device)
decoder_net=Decoder(len_token_map[1],max_size_E_D[1],train_L[2],len_token_map[1],train_L[3],train_L[4],p=dec_dropout).to(device)
model_L = [encoder_net,decoder_net,cell_type]
model = Seq2Seq(model_L[0],model_L[1],model_L[2]).to(device)
criterion = nn.CrossEntropyLoss()


optimizer = optim.Adam(model.parameters(),lr=learning_rate)
def train(model: nn.Module, iterator: torch.utils.data.DataLoader, optimizer: optim.Optimizer, criterion: nn.Module, clip: float):


    epoch_loss, epoch_acc = 0, 0
    model.train()


    iterator_iter = iter(iterator)
    index = 0
    while True:
        try:
            src, trg = next(iterator_iter)
            optimizer.zero_grad()
            src = src.to(device)
            trg = trg.to(device)
            output = model(src, trg)

            output = output[1:].view(-1, output.shape[-1])
            trg = trg.permute(1,0)
            trg = torch.reshape(trg[1:], (-1,))
            loss = criterion(output, trg)

            # calculate accuracy
            preds = torch.argmax(output, dim=1)

            non_zero_elements = (trg != 0)
            non_zero_indices = non_zero_elements.nonzero(as_tuple=True)
            non_pad_elements = non_zero_indices[0]


            preds_non_pad = preds[non_pad_elements]
            trg_non_pad = trg[non_pad_elements]
            correct = preds_non_pad == trg_non_pad

            correct_sum = correct.sum().item()
            non_pad_length = len(non_pad_elements)
            accuracy = correct_sum / non_pad_length
            epoch_acc = epoch_acc + accuracy

            loss.backward()

            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)

            optimizer.step()

            epoch_loss = epoch_loss + loss.item()
            index += 1
        except StopIteration:
            break
    loss_actual = epoch_loss / len(iterator)
    accuracy_actual = epoch_acc / len(iterator)
    return  loss_actual, accuracy_actual


def evaluate(model: nn.Module,iterator: torch.utils.data.DataLoader,criterion: nn.Module):
    epoch_loss, epoch_acc = 0, 0

    solution=[]
    model.eval()

    with torch.no_grad():
        iterator_iter = iter(iterator)

        index = 0
        while True:
          try:
            src, trg = next(iterator_iter)
            src = src.to(device)
            trg = trg.to(device)

            output = model(src, trg, 0) #turn off teacher forcing
            output = output[1:].view(-1, output.shape[-1])

            trg = trg.permute(1,0)
            trg = torch.reshape(trg[1:], (-1,))

            loss = criterion(output, trg)

            epoch_loss = epoch_loss + loss.item()
            # calculate accuracy
            preds = torch.argmax(output, dim=1)
            matrix_size = [16, 29]
            b=np.zeros((matrix_size[0],matrix_size[1]))
            i = 0
            while i < 16:
                j = 0
                while j < 29:
                    b[i][j] = preds[16 * j + i]
                    j += 1
                i += 1

            i = 0
            while i < 16:
              solution.append(reverse_tokenize(b[i]))
              i+=1

            non_zero_elements = trg != 0
            non_zero_indices = non_zero_elements.nonzero(as_tuple=True)
            non_pad_elements = non_zero_indices[0]


            preds_non_pad = preds[non_pad_elements]
            trg_non_pad = trg[non_pad_elements]
            correct = preds_non_pad == trg_non_pad


            correct_sum = correct.sum().item()
            non_pad_length = len(non_pad_elements)
            accuracy = correct_sum / non_pad_length
            epoch_acc += accuracy
            index += 1
          except StopIteration:
            break
    loss_actual = epoch_loss / len(iterator)
    accuracy_actual = epoch_acc / len(iterator)
    return  loss_actual, accuracy_actual,solution




def main(N_EPOCHS , CLIP):
  best_valid_loss = float('inf')
  epoch = 0

  while epoch < N_EPOCHS:
      train_loss, train_acc = train(model, train_iter, optimizer, criterion, CLIP)
      valid_loss, val_acc, solution = evaluate(model, valid_iter, criterion)
      print(f'Epoch: {epoch+1:02}')
      print(f'\tTrain Loss: {train_loss:.3f} | Train accuracy: {train_acc*33:.3f}')
      print(f'\t Val. Loss: {valid_loss:.3f} | Val accuracy: {val_acc*33:.3f}')
      epoch+=1


  test_loss,test_accuracy,solution_test = evaluate(model, test_iter, criterion)

  print(f'| Test Loss: {test_loss:.3f} | Test accuracy: {test_accuracy*33:7.3f} |')

sweep_configuration={

    'name':'cs23m074',
    'method':'bayes',
    'metric':{'name':'val_acc','goal':'maximize'},
    'parameters':{

        'epochs':{
            'values':[1]
        },

        'embedding_size':{
            'values':[64,128,256]
            },

        'dropout':{
            'values':[0, 0.2,0.3 , 0.4 , 0.6]
            },


        }
    }

# Initialize wandb sweep based on sweep_configuration
sweep_id = wandb.sweep(sweep_configuration, project=args.wandb_project)
# Check if logging with wandb is enabled
if args.logger: #log in wandb
  wandb.init(config=args, project=args.wandb_project, entity=args.wandb_entity, reinit='true')
  wandb.finish()
# Load pre-trained model or initialize new model
load_model = True
config = wandb.config  # Assuming you want to load a pre-trained model


# Display parsed arguments
print("CONFIGURATIONS =>")
print("Wandb Project:", args.wandb_project)
print("Wandb Entity:", args.wandb_entity)
print("clip:", args.clip)
print("Epochs:", args.epochs)
print("hidden layer size_filter:", args.hidden_layer_size)
print("Dropout:", args.dropout)
print("embedding size:", args.embedding_size)

# Call main function with specified arguments
main(args.epochs , 1)