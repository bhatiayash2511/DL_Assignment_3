# -*- coding: utf-8 -*-
"""trainofattention-py (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P75atiPGBu2b59BEO1iexQ6MHqpuipML
"""

# !pip install wandb

# from google.colab import drive
# drive.mount('/content/drive')

import pandas as pd
import argparse
import numpy as np
import random
from typing import Tuple
import torch
import torch.nn as nn
import torch.optim as optim
import string
import random
from collections import Counter
import torch
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader
SEED = random.randint(1, 1000)
random.seed(SEED)
torch.manual_seed(SEED)
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch import Tensor
import pandas as pd
import wandb
import torch
unknown_token=64

BATCH_SIZE = 512
N_EPOCHS = 1
CLIP = 1
emb_dim = 256
hidden_layer_dim = 256
attn_dim = 128
dropout = 0.6
opti = "adam"

# ///////////////////////////////////////////////////////////
# if __name__ == "__main__":
# Initialize argument parser
parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
# Define arguments for hyperparameters
parser.add_argument("-wp", '--wandb_project', help='Project name used to track experiments in Weights & Biases dashboard', type=str, default='DL_A3')
parser.add_argument("-we", "--wandb_entity", type=str, help="Wandb Entity used to track experiments in the Weights & Biases dashboard.", default="cs23m013")
parser.add_argument('-ep', '--epochs', help="Number of epochs to train neural network.", type=int, default=10)
parser.add_argument("-dp", "--dropout", default=0.2, type=float, choices=[0, 0.2,0.3 , 0.4 , 0.6])
parser.add_argument("-lg", "--logger", type=bool, default=False, choices=[True, False], help="Log to wandb or not")

parser.add_argument('-tpd', '--test_path_directory', help="Dataset", type=str, default='/content/drive/MyDrive/DL/A3_Data/aksharantar_sampled/mar/mar_test.csv')
parser.add_argument('-trpd', '--train_path_directory', help="Dataset", type=str, default='/content/drive/MyDrive/DL/A3_Data/aksharantar_sampled/mar/mar_train.csv')
parser.add_argument('-vpd', '--val_path_directory', help="Dataset", type=str, default='/content/drive/MyDrive/DL/A3_Data/aksharantar_sampled/mar/mar_valid.csv')

parser.add_argument('-cp', '--clip', help='choices: [1,2,3]', choices=[1,2,3], type=int, default=1)
parser.add_argument("-hls", "--hidden_layer_size", default=256, type=int, choices=[64,128 , 256,512])
parser.add_argument("-es", "--embedding_size", type=int, default=256 , choices=[64,128,256])


# Parse arguments from command line
args = parser.parse_args()
file_paths = [
    args.train_path_directory,
    args.val_path_directory,
    args.test_path_directory
]

# /////////////////////////////////////////////////////////////////

def load_data(path):
    data = pd.read_csv(path, header=None, names=["en", "ma", ""], skip_blank_lines=True)
    data = data.dropna(subset=['en', 'ma'])
    return data[['en', 'ma']]

# Define file paths
# By default
BATCH_SIZE = 512
N_EPOCHS = 1
CLIP = 1
emb_dim = 256
hidden_layer_dim = 256
attn_dim = 128
dropout = 0.6
opti = "adam"

# Load data for train, val, and test
train, val, test = [load_data(path) for path in file_paths]

def unique_tokenize(data):
    english_tokens = set(''.join(data['en']))
    marathi_tokens = set(''.join(data['ma']))
    return sorted(list(marathi_tokens)), sorted(list(english_tokens))

marathi_tokens , english_tokens = unique_tokenize(train)

def tokenize_map(hindi_tokens, english_tokens):
    # Create token maps for English and Marathi
    english_token_map = {token: idx + 1 for idx, token in enumerate(english_tokens)}
    marathi_token_map = {token: idx + 1 for idx, token in enumerate(hindi_tokens)}

    # Create reverse token map for Marathi
    reverse_marathi_token_map = {idx + 1: token for idx, token in enumerate(hindi_tokens)}


    marathi_token_map[" "] = 0
    english_token_map[" "] = 0
    reverse_marathi_token_map[65] = ';'
    marathi_token_map[';'] = 65
    reverse_marathi_token_map[0] = ''
    marathi_token_map['<unk>'] = 64
    english_token_map[';'] = 27
    reverse_marathi_token_map[64] = '<unk>'
    reverse_marathi_token_map[66] = '.'
    english_token_map['.'] = 28
    marathi_token_map['.'] = 66


    return marathi_token_map, english_token_map, reverse_marathi_token_map



mar_token_map, eng_token_map,reverse_marathi_token_map = tokenize_map(marathi_tokens , english_tokens)
x = test['en'].values
y = test['ma'].values


max_eng_len = max(len(sentence) for sentence in x) + 2
max_mar_len = max(len(sentence) for sentence in y) + 2

def process(data):
    x, y = data['en'].values, data['ma'].values

    # Add start and end tokens to sentences
    x = [';' + sentence + '.' for sentence in x]
    y = [';' + sentence + '.' for sentence in y]
#     print(x[0:3])
#     print(y[0:3])
    a = torch.zeros((len(x), max_eng_len), dtype=torch.int64)
    b = torch.zeros((len(y), max_eng_len), dtype=torch.int64)

    # Process data
    for i, (xx, yy) in enumerate(zip(x, y)):
        for j, ch in enumerate(xx):
            a[i, j] = eng_token_map.get(ch, unknown_token)

        for j, ch in enumerate(yy):
            b[i, j] = mar_token_map.get(ch, unknown_token)


    # Create list of tuples (input tensor, target tensor)
    data = [(a[i], b[i]) for i in range(len(x))]

    return data


train_process=process(train)
val_process=process(val)
test_process=process(test)



device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# print(device)

def dataL(P, BATCH_SIZE, shuffle_bool):
    iter = DataLoader(P, batch_size=BATCH_SIZE, shuffle=shuffle_bool)

    return iter



train_iter = dataL(train_process, BATCH_SIZE, False)
valid_iter = dataL(val_process, BATCH_SIZE, False)
test_iter = dataL(test_process, BATCH_SIZE, False)

class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):
        super(Encoder, self).__init__()
        l = [input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout]
        self.input_dim = l[0]
        self.emb_dim = l[1]
        self.enc_hid_dim = l[2]
        self.dec_hid_dim = l[3]
        self.dropout = l[4]

        self.embedding = nn.Embedding(l[0], l[1])
        bidir_bool = True   # Always True
        # GRU
        self.rnn = nn.GRU(l[1], l[2], bidirectional=bidir_bool)
        temp = enc_hid_dim * 2
        self.fc = nn.Linear(temp, l[3])
        self.dropout = nn.Dropout(l[4])


    def forward(self, src: Tensor) -> Tuple[Tensor]:
        # Transpose the source tensor
        src_transposed = src.transpose(0, 1)

        # Embed the transposed source tensor with dropout
        embedded = self.dropout(self.embedding(src_transposed))

        # Pass the embedded tensor through the RNN
        outputs, hidden = self.rnn(embedded)

        # Concatenate and apply tanh activation to the last two layers of hidden states
        last_hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)

        linear_transform = self.fc(last_hidden)
        hidden_combined = torch.tanh(linear_transform)
        return outputs, hidden_combined

class Attention(nn.Module):

    def __init__(self, enc_hid_dim: int, dec_hid_dim: int, attn_dim: int):
        super(Attention, self).__init__()
        hid_l = [enc_hid_dim, dec_hid_dim]
        self.enc_hid_dim = hid_l[0]
        self.dec_hid_dim = hid_l[1]
        enc_hid_dim_double = 2 * enc_hid_dim
        temp = enc_hid_dim_double + dec_hid_dim
        self.attn_in = temp

        self.attn = nn.Linear(self.attn_in, attn_dim)


    def forward(self, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tensor:
        src_len = encoder_outputs.size(0)

        decoder_hidden_expanded = decoder_hidden.unsqueeze(1)

        repeated_decoder_hidden = decoder_hidden_expanded.repeat(1, src_len, 1)

        encoder_outputs = encoder_outputs.permute(1, 0, 2)

        concat_hidden_outputs = torch.cat((repeated_decoder_hidden, encoder_outputs), dim=2)
        linear_transform = self.attn(concat_hidden_outputs)
        energy = torch.tanh(linear_transform)
        attention = torch.sum(energy, dim=2)

        attention_weights = F.softmax(attention, dim=1)

        return attention_weights

class Decoder(nn.Module):
    def __init__(self, output_dim: int, emb_dim: int, enc_hid_dim: int,
                 dec_hid_dim: int, dropout: float, attention: nn.Module):
        super().__init__()
        cons_l = [output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention]
        self.output_dim = cons_l[0]
        self.emb_dim = cons_l[1]
        self.enc_hid_dim =cons_l[2]
        self.dec_hid_dim = cons_l[3]
        self.dropout_p = cons_l[4]
        self.attention = cons_l[5]

        self.embedding = nn.Embedding(cons_l[0], cons_l[1])
        self.rnn_input_size = cons_l[2] * 2 + cons_l[1]
        self.rnn = nn.GRU(self.rnn_input_size, cons_l[3])
        self.out_input_size = self.attention.attn_in + cons_l[1]
        self.out = nn.Linear(self.out_input_size, cons_l[0])
        self.dropout = nn.Dropout(cons_l[4])



    def _weighted_encoder_rep(self, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tensor:
        # Calculate attention weights
        attention_weights = self.attention(decoder_hidden, encoder_outputs)
        attention_weights = attention_weights.unsqueeze(1)  # Add a dimension for batch multiplication

        # Transpose encoder outputs for correct alignment
        encoder_outputs = encoder_outputs.permute(1, 0, 2)

        # Calculate weighted encoder representation
        weighted_encoder_rep = torch.bmm(attention_weights, encoder_outputs)
        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)  # Transpose back for output

        return weighted_encoder_rep

    def forward(self, input: Tensor, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tuple[Tensor]:
        # Add a batch dimension to input
        input = input.unsqueeze(0)

        # Transpose input tensor
        input = input.permute(1, 0)

        # Apply embedding to the input tensor
        embedded_input = self.embedding(input)

        # Apply dropout to the embedded input tensor
        embedded = self.dropout(embedded_input)

        # Calculate the weighted encoder representation
        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden, encoder_outputs)

        # Transpose embedded tensor for concatenation
        embedded = embedded.permute(1, 0, 2)

        # Concatenate embedded tensor and weighted encoder representation
        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim=2)

        # Pass through the GRU layer
        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))

        # Squeeze the tensors to remove batch dimension
        embedded = embedded.squeeze(0)
        output = output.squeeze(0)
        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)

        # Concatenate output, weighted encoder representation, and embedded tensor
        concatenated_output = torch.cat((output, weighted_encoder_rep, embedded), dim=1)

        # Pass through the output layer
        final_output = self.out(concatenated_output)

        return final_output, decoder_hidden.squeeze(0)

class Seq2Seq(nn.Module):
    def __init__(self,encoder: nn.Module,decoder: nn.Module, device: torch.device):
        super().__init__()

        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self,src: Tensor,trg: Tensor, teacher_forcing_ratio: float = 0.5) -> Tensor:

        batch_size = src.shape[0]
        max_len = trg.shape[1]
        trg_vocab_size = self.decoder.output_dim
        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)
        #ATTENTION HEAT MAP METHOD
        #attentions = torch.zeros(max_len, batch_size, src.shape[1]).to(self.device)

        encoder_outputs, hidden = self.encoder(src)

        # first input to the decoder is the <sos> token
        trg = trg.permute(1,0)
        output = trg[0,:]

        t = 1
        while t < max_len:
            output, hidden = self.decoder(output, hidden, encoder_outputs)
            outputs[t] = output
            if random.random() < teacher_forcing_ratio:
                output = trg[t]
            else:
                output = output.max(1)[1]
            t += 1


        return outputs

def init_weights(m: nn.Module):
    params = iter(m.named_parameters())
    try:
        while True:
            name, param = next(params)
            if 'weight' in name:
                nn.init.normal_(param.data, mean=0, std=0.01)
            else:
                nn.init.constant_(param.data, 0)
    except StopIteration:
        pass

def count_parameters(model: nn.Module):
    total_params = 0
    for p in model.parameters():
        if p.requires_grad:
            total_params += p.numel()
    return total_params


def train_model(model, train_iter, optimizer, criterion, device, CLIP):
    model.train()
    train_epoch_loss = 0
    train_epoch_acc = 0
    train_iter_iter = iter(train_iter)
    index = 0
    while True:
        try:
            _, (src, trg) = next(enumerate(train_iter_iter))

            optimizer.zero_grad()
            trg = trg.to(device)
            src = src.to(device)

            output = model(src, trg)

            output = output[1:].view(-1, output.shape[-1])
            trg = trg.permute(1, 0)
            trg = torch.reshape(trg[1:], (-1,))
            loss = criterion(output, trg)

            preds = torch.argmax(output, dim=1)

            non_zero_elements = trg != 0
            non_zero_indices = non_zero_elements.nonzero(as_tuple=True)
            non_pad_elements = non_zero_indices[0]

            preds_non_pad = preds[non_pad_elements]
            trg_non_pad = trg[non_pad_elements]
            train_correct = preds_non_pad == trg_non_pad

            train_correct_sum = train_correct.sum().item()
            non_pad_length = len(non_pad_elements)
            accuracy = train_correct_sum / non_pad_length
            train_epoch_acc += accuracy

            loss.backward()

            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)

            optimizer.step()

            train_epoch_loss += loss.item()
            index += 1
        except StopIteration:
            break

    train_epoch_loss /= len(train_iter)
    train_epoch_acc /= len(train_iter)

    return train_epoch_loss, train_epoch_acc

def evaluate_model(model, valid_iter, criterion, device):
    model.eval()
    val_epoch_loss = 0
    val_epoch_acc = 0

    with torch.no_grad():

        for _, (src, trg) in enumerate(valid_iter):
                # Your code for processing src and trg goes here

                src, trg = src.to(device), trg.to(device)

                output = model(src, trg, 0)

                output = output[1:].view(-1, output.shape[-1])
                trg = trg.permute(1, 0)
                trg = torch.reshape(trg[1:], (-1,))

                loss = criterion(output, trg)

                val_epoch_loss = val_epoch_loss + loss.item()

                preds = torch.argmax(output, dim=1)

                non_zero_elements = trg != 0
                non_zero_indices = non_zero_elements.nonzero(as_tuple=True)
                non_pad_elements = non_zero_indices[0]


                preds_non_pad = preds[non_pad_elements]
                trg_non_pad = trg[non_pad_elements]
                val_correct = preds_non_pad == trg_non_pad
                val_correct_sum = val_correct.sum().item()
                non_pad_length = len(non_pad_elements)
                accuracy = val_correct_sum / non_pad_length
                val_epoch_acc += accuracy




    val_epoch_loss /= len(valid_iter)
    val_epoch_acc /= len(valid_iter)

    return val_epoch_loss, val_epoch_acc



def run_training_loop(model, train_iter, valid_iter, optimizer, criterion, device, CLIP, N_EPOCHS):
    for epoch in range(N_EPOCHS):
        train_epoch_loss, train_epoch_acc = train_model(model, train_iter, optimizer, criterion, device, CLIP)
        if epoch % 5 == 0:
            val_epoch_loss, val_epoch_acc = evaluate_model(model, valid_iter, criterion, device)

            print(f'Epoch: {epoch+1:02} | Train accuracy: {train_epoch_acc*100:.3f} | test/val which ever you pass accuracy: {val_epoch_acc*100:.3f}')
            #         wandb.log({"train_loss": train_epoch_loss, "train_accuracy": train_epoch_acc, "val_loss": val_epoch_loss, "val_accuracy": val_epoch_acc})

        if epoch == N_EPOCHS - 1:
            val_epoch_loss, val_epoch_acc = evaluate_model(model, valid_iter, criterion, device)
            print(f'Epoch: {epoch+1:02} | Train accuracy: {train_epoch_acc*100:.3f} | test/val which ever you pass accuracy: {val_epoch_acc*100:.3f}')

            torch.cuda.empty_cache()



token_map_list_length = [len(eng_token_map), len(mar_token_map)]
INPUT_DIM = token_map_list_length[0]
OUTPUT_DIM = token_map_list_length[1]
enc_param = [INPUT_DIM, emb_dim, hidden_layer_dim, hidden_layer_dim, dropout]
enc = Encoder(enc_param[0], enc_param[1],enc_param[2],enc_param[3],enc_param[4])
att_param = [hidden_layer_dim, hidden_layer_dim, attn_dim]
attn = Attention(att_param[0], att_param[1], att_param[2])
dec_param = [OUTPUT_DIM, emb_dim, hidden_layer_dim, hidden_layer_dim, dropout, attn]
dec = Decoder(dec_param[0],dec_param[1],dec_param[2],dec_param[3],dec_param[4],dec_param[5])

model = Seq2Seq(enc, dec, device).to(device)
model.apply(init_weights)

if opti == "adam":
    optimizer = optim.Adam(model.parameters())
if opti == "rmsprop":
    optimizer = optim.RMSprop(model.parameters())
if opti == "adagrad":
    optimizer = optim.Adagrad(model.parameters())
else:
    optimizer = optim.Adam(model.parameters())

criterion = nn.CrossEntropyLoss()

model.train()

train_epoch_loss = 0
train_epoch_acc = 0
val_epoch_loss = 0
val_epoch_acc = 0

# run_training_loop(model, train_iter, test_iter, optimizer, criterion, device, CLIP, N_EPOCHS)

sweep_configuration={

    'name':'cs23m013',
    'method':'bayes',
    'metric':{'name':'val_acc','goal':'maximize'},
    'parameters':{

        'epochs':{
            'values':[1]
        },

         'learning_rate':{
            'values':[0.001 , 0.0001 , 0.00001]
        },

        'embedding_size':{
            'values':[64,128,256]
            },

        'num_encoder_layer':{
            'values':[1,2,3]
            },

        'num_decoder_layer':{
            'values':[1,2,3]
        },

        'hidden_layer_size':{
            'values':[64,128 , 256,512]
            },

        'cell_type':{
            'values':['LSTM','GRU', 'RNN']
            },

        'dropout':{
            'values':[0, 0.2,0.3 , 0.4 , 0.6]
            },

        'bidirection':{
            'values':[True,False]
            }

        }
    }

# Initialize wandb sweep based on sweep_configuration
sweep_id = wandb.sweep(sweep_configuration, project=args.wandb_project)
# Check if logging with wandb is enabled
if args.logger: #log in wandb
  wandb.init(config=args, project=args.wandb_project, entity=args.wandb_entity, reinit='true')
  wandb.finish()
# Load pre-trained model or initialize new model
load_model = True
config = wandb.config  # Assuming you want to load a pre-trained model

BATCH_SIZE = 512
N_EPOCHS = 1
CLIP = 1
emb_dim = 256
hidden_layer_dim = 256
attn_dim = 128
dropout = 0.6
opti = "adam"


model = Seq2Seq(enc, dec, device).to(device)
model.apply(init_weights)

if opti == "adam":
    optimizer = optim.Adam(model.parameters())
criterion = nn.CrossEntropyLoss()

model.train()



# Display parsed arguments
print("CONFIGURATIONS =>")
print("Wandb Project:", args.wandb_project)
print("Wandb Entity:", args.wandb_entity)
print("clip:", args.clip)
print("Epochs:", args.epochs)
print("hidden layer size_filter:", args.hidden_layer_size)
print("Dropout:", args.dropout)
print("embedding size:", args.embedding_size)

# Call main function with specified arguments
run_training_loop(model, train_iter, test_iter, optimizer, criterion, device, CLIP,  args.epochs)