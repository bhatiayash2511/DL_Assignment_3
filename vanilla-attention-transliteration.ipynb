{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0029bd3ca1c94350a4ed311349e040bf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"009d917192ac45f8b9a5cb3e88b3be67":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09dc089336be4b8a9fb06ebd4e27b92e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c4c5300439a47fca5d0dc2bbb00c7c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e20596ffc514c82b89c308c78e60097":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_009d917192ac45f8b9a5cb3e88b3be67","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_99445299e74d4e7db3951662a977c21a","value":0.11012801204819277}},"2de8fbf1431348c0b375c2bea37bc58a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f6b8df6ebb44ab1b38f5aebd30225d5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fb0c0dc7f564ad5849f01201a2349dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_fafea03cb3874a209280303fd0ac44e5","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_66c1662f065e4171803abdb4210d4df0","value":1}},"3726b5cd9ab240f198b0abf2bd0ee999":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cb5d0076b4e4ce8bf8943a44895bdc6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_ee966b1121cd45c28893b2cc3a0f36ef","IPY_MODEL_1e20596ffc514c82b89c308c78e60097"],"layout":"IPY_MODEL_09dc089336be4b8a9fb06ebd4e27b92e"}},"3f8878c3e01f4dfe954752b6a726974c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5cffdb42030b4aaaa4adf32ed67a4bd0","placeholder":"​","style":"IPY_MODEL_d490c128677e486981c7ed932e7a8e4a","value":"Waiting for wandb.init()...\r"}},"3fa2e79d8bf645fd925b2ca74b33ace1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4149a4c78e664d71a256e6deaab7716d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_3f8878c3e01f4dfe954752b6a726974c","IPY_MODEL_2fb0c0dc7f564ad5849f01201a2349dc"],"layout":"IPY_MODEL_3726b5cd9ab240f198b0abf2bd0ee999"}},"4562bbd5ab014f7084a5b74100662d6c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4db95218bd8c45fd8638e73b17655541":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e3528158aaf4a6687bdeb93fd7164d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5cffdb42030b4aaaa4adf32ed67a4bd0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62351b579cad492bafe79bec945e8327":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fa2e79d8bf645fd925b2ca74b33ace1","placeholder":"​","style":"IPY_MODEL_2de8fbf1431348c0b375c2bea37bc58a","value":"0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\r"}},"62cdf5406f4f4ae0bfef029acc8b25ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_0029bd3ca1c94350a4ed311349e040bf","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_99b20e257eb94f7c9ea138b138b8b92a","value":1}},"66c1662f065e4171803abdb4210d4df0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"770c3cb14d53452296ae96b6d12acb2b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99445299e74d4e7db3951662a977c21a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"99b20e257eb94f7c9ea138b138b8b92a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c269d71a857e408aa28d91b2efd35eff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4204db901f44e57a468b949be7a9cc5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0e13e633f6c46749d354d038df3dfff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_4562bbd5ab014f7084a5b74100662d6c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4e3528158aaf4a6687bdeb93fd7164d3","value":1}},"d2c4d6f23f594ef3b9bb9514eb986abb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_62351b579cad492bafe79bec945e8327","IPY_MODEL_62cdf5406f4f4ae0bfef029acc8b25ac"],"layout":"IPY_MODEL_770c3cb14d53452296ae96b6d12acb2b"}},"d490c128677e486981c7ed932e7a8e4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8e252f6d5d141bba53ec2757bbb498f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_ed6fb297d7cb4188953724e8fd678c56","IPY_MODEL_d0e13e633f6c46749d354d038df3dfff"],"layout":"IPY_MODEL_2f6b8df6ebb44ab1b38f5aebd30225d5"}},"ed6fb297d7cb4188953724e8fd678c56":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4db95218bd8c45fd8638e73b17655541","placeholder":"​","style":"IPY_MODEL_c4204db901f44e57a468b949be7a9cc5","value":"0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\r"}},"ee966b1121cd45c28893b2cc3a0f36ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c269d71a857e408aa28d91b2efd35eff","placeholder":"​","style":"IPY_MODEL_1c4c5300439a47fca5d0dc2bbb00c7c5","value":"0.001 MB of 0.010 MB uploaded (0.000 MB deduped)\r"}},"fafea03cb3874a209280303fd0ac44e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8426411,"sourceType":"datasetVersion","datasetId":5017458}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch.optim as optim\nimport pandas as pd\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\n\nimport math\nimport time\nimport pandas as pd\nimport numpy as np\nimport random\nfrom typing import Tuple\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport string\nimport random\nfrom collections import Counter\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fl3oO7Gd7Yi7","outputId":"168a1e6c-a21f-42ee-e706-4358a15c0454","execution":{"iopub.status.busy":"2024-05-17T07:57:37.189626Z","iopub.execute_input":"2024-05-17T07:57:37.189928Z","iopub.status.idle":"2024-05-17T07:57:37.198689Z","shell.execute_reply.started":"2024-05-17T07:57:37.189904Z","shell.execute_reply":"2024-05-17T07:57:37.197934Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# LOADING DATA FROM CSV FILES","metadata":{"id":"5NDoNJFeN5TC"}},{"cell_type":"code","source":"def load_data(path):\n    with open(path) as fil:\n        data = pd.read_csv(fil,sep=',',header=None,names=[\"en\",\"ma\",\"\"],skip_blank_lines=True,index_col=None)\n    data = data[data['en'].notna()]\n    data = data[data['ma'].notna()]\n    data = data[['en','ma']]\n    return data","metadata":{"id":"8DUylqRTCSHJ","execution":{"iopub.status.busy":"2024-05-17T07:57:37.199961Z","iopub.execute_input":"2024-05-17T07:57:37.200624Z","iopub.status.idle":"2024-05-17T07:57:37.210847Z","shell.execute_reply.started":"2024-05-17T07:57:37.200591Z","shell.execute_reply":"2024-05-17T07:57:37.210120Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train = load_data(\"/kaggle/input/marathi-dataset/mar/mar_train.csv\")\nval = load_data(\"/kaggle/input/marathi-dataset/mar/mar_valid.csv\")\ntest = load_data(\"/kaggle/input/marathi-dataset/mar/mar_test.csv\")\n\n","metadata":{"id":"drRm1kLlCdXy","execution":{"iopub.status.busy":"2024-05-17T07:57:37.211874Z","iopub.execute_input":"2024-05-17T07:57:37.212163Z","iopub.status.idle":"2024-05-17T07:57:37.356484Z","shell.execute_reply.started":"2024-05-17T07:57:37.212140Z","shell.execute_reply":"2024-05-17T07:57:37.355437Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"51200\n4096\n4096\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#TOKENIZE DATASET","metadata":{"id":"feSI-Xyl-DM2"}},{"cell_type":"code","source":"def unique_tokenize(data):\n    english = train['en'].values\n    marathi = train['ma'].values\n    english_tokens = set()\n    marathi_tokens = set()\n    \n    for x,y in zip(english,marathi):\n        for ch in x:\n            english_tokens.add(ch)\n        for ch in y:\n            marathi_tokens.add(ch)\n    english_tokens = sorted(list(english_tokens))\n    marathi_tokens = sorted(list(marathi_tokens))\n    return marathi_tokens , english_tokens\nmarathi_tokens , english_tokens = unique_tokenize(train)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g3t82UK3kgyF","outputId":"a5051068-7fad-4e6c-f9a8-2068603d5429","execution":{"iopub.status.busy":"2024-05-17T07:57:37.359223Z","iopub.execute_input":"2024-05-17T07:57:37.359540Z","iopub.status.idle":"2024-05-17T07:57:37.508713Z","shell.execute_reply.started":"2024-05-17T07:57:37.359507Z","shell.execute_reply":"2024-05-17T07:57:37.507956Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n63\n","output_type":"stream"}]},{"cell_type":"code","source":"#REWRITE AGAIN\ndef tokenize_map(hindi_tokens , english_tokens):\n    english_token_map = dict([(ch,i+1) for i,ch in enumerate(english_tokens)])\n    marathi_token_map = dict([(ch,i+1) for i,ch in enumerate(marathi_tokens)])\n    reverse_marathi_token_map = dict([(i+1,ch) for i,ch in enumerate(marathi_tokens)])\n    #Adding blank space\n\n    marathi_token_map[\" \"] = 0\n    english_token_map[\" \"] = 0\n    #addin BOS and EOS token \n    marathi_token_map[';']=65\n    marathi_token_map['.']=66\n    english_token_map[';']=27\n    english_token_map['.']=28\n    marathi_token_map['<unk>']=64\n    \n    reverse_marathi_token_map[64]='<unk>'\n    reverse_marathi_token_map[65]=';'\n    reverse_marathi_token_map[66]='.'\n    reverse_marathi_token_map[0]=''\n\n    return marathi_token_map, english_token_map,reverse_marathi_token_map\n\nmar_token_map, eng_token_map,reverse_marathi_token_map = tokenize_map(marathi_tokens , english_tokens)\nprint(mar_token_map)\nprint(reverse_marathi_token_map)\nprint(eng_token_map)\n\nprint('mar:',len(mar_token_map))\nprint('eng:',len(eng_token_map))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yeeJVQ2EjRqO","outputId":"2b5de9b9-6673-4f4c-e90e-a3722d9f8aed","execution":{"iopub.status.busy":"2024-05-17T07:57:37.509771Z","iopub.execute_input":"2024-05-17T07:57:37.510018Z","iopub.status.idle":"2024-05-17T07:57:37.519366Z","shell.execute_reply.started":"2024-05-17T07:57:37.509997Z","shell.execute_reply":"2024-05-17T07:57:37.518464Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"{'ँ': 1, 'ं': 2, 'ः': 3, 'अ': 4, 'आ': 5, 'इ': 6, 'ई': 7, 'उ': 8, 'ऊ': 9, 'ऋ': 10, 'ऍ': 11, 'ए': 12, 'ऐ': 13, 'ऑ': 14, 'ओ': 15, 'औ': 16, 'क': 17, 'ख': 18, 'ग': 19, 'घ': 20, 'च': 21, 'छ': 22, 'ज': 23, 'झ': 24, 'ञ': 25, 'ट': 26, 'ठ': 27, 'ड': 28, 'ढ': 29, 'ण': 30, 'त': 31, 'थ': 32, 'द': 33, 'ध': 34, 'न': 35, 'प': 36, 'फ': 37, 'ब': 38, 'भ': 39, 'म': 40, 'य': 41, 'र': 42, 'ल': 43, 'ळ': 44, 'व': 45, 'श': 46, 'ष': 47, 'स': 48, 'ह': 49, '़': 50, 'ा': 51, 'ि': 52, 'ी': 53, 'ु': 54, 'ू': 55, 'ृ': 56, 'ॅ': 57, 'े': 58, 'ै': 59, 'ॉ': 60, 'ो': 61, 'ौ': 62, '्': 63, ' ': 0, ';': 65, '.': 66, '<unk>': 64}\n{1: 'ँ', 2: 'ं', 3: 'ः', 4: 'अ', 5: 'आ', 6: 'इ', 7: 'ई', 8: 'उ', 9: 'ऊ', 10: 'ऋ', 11: 'ऍ', 12: 'ए', 13: 'ऐ', 14: 'ऑ', 15: 'ओ', 16: 'औ', 17: 'क', 18: 'ख', 19: 'ग', 20: 'घ', 21: 'च', 22: 'छ', 23: 'ज', 24: 'झ', 25: 'ञ', 26: 'ट', 27: 'ठ', 28: 'ड', 29: 'ढ', 30: 'ण', 31: 'त', 32: 'थ', 33: 'द', 34: 'ध', 35: 'न', 36: 'प', 37: 'फ', 38: 'ब', 39: 'भ', 40: 'म', 41: 'य', 42: 'र', 43: 'ल', 44: 'ळ', 45: 'व', 46: 'श', 47: 'ष', 48: 'स', 49: 'ह', 50: '़', 51: 'ा', 52: 'ि', 53: 'ी', 54: 'ु', 55: 'ू', 56: 'ृ', 57: 'ॅ', 58: 'े', 59: 'ै', 60: 'ॉ', 61: 'ो', 62: 'ौ', 63: '्', 64: '<unk>', 65: ';', 66: '.', 0: ''}\n{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, ' ': 0, ';': 27, '.': 28}\nmar: 67\neng: 29\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#MAXIMUM WORD LENGTH THAT ARE PRESENT IN THE DATASET\n\n\n","metadata":{"id":"MOOqbrfKo-ks"}},{"cell_type":"code","source":"\nx = test['en'].values\ny = test['ma'].values\n\n#Getting max length\nmax_eng_len = max([len(i) for i in x]) + 2\nmax_mar_len = max([len(i) for i in y]) + 2\nprint(max_eng_len,max_mar_len)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1aMkFt73o-IW","outputId":"ab6a2141-8fb1-4167-985d-2693cd06d1a2","execution":{"iopub.status.busy":"2024-05-17T07:57:37.520811Z","iopub.execute_input":"2024-05-17T07:57:37.521150Z","iopub.status.idle":"2024-05-17T07:57:37.535087Z","shell.execute_reply.started":"2024-05-17T07:57:37.521121Z","shell.execute_reply":"2024-05-17T07:57:37.534337Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"30 22\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ONE HOT ENCODING/EMBEDDING OUR INPUT\n\n","metadata":{"id":"i-ry8u0ousdZ"}},{"cell_type":"code","source":"\nimport torch\n#unknown token present in validation set as 'r.(in marathi)'\nunknown_token=64\ndef process(data):\n    x,y = data['en'].values, data['ma'].values\n    x = \";\" + x + \".\"\n    y = \";\" + y + \".\"\n    print(x[0:3])\n    print(y[0:3]) \n    \n    a = torch.zeros((len(x),max_eng_len),dtype=torch.int64)\n    print(a.shape)\n    \n    b = torch.zeros((len(y),max_eng_len),dtype=torch.int64)\n    \n    data=[]\n    for i,(xx,yy) in enumerate(zip(x,y)):\n        for j,ch in enumerate(xx):\n            a[i,j] = eng_token_map[ch]\n\n        #a[i,j+1:] = eng_token_map[\" \"]\n        for j,ch in enumerate(yy):\n            if ch in mar_token_map: \n             b[i,j] = mar_token_map[ch]\n            else:\n              b[i,j]= unknown_token\n\n\n    data = [(a[i], b[i]) for i in range(len(x))]\n    print(a.shape)\n    print(b.shape)\n    return data\n\ntrain_process=process(train)\nval_process=process(val)\ntest_process=process(test)\n#print(train_process.shape)\nprint('\\n')\nprint('num of rows:',len(train_process))\nprint('num of columns:',len(train_process[0]))\nprint(train_process[0][1])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fdffrLwysNo-","outputId":"0f61f3f7-1140-4de5-a547-90f9019972c6","execution":{"iopub.status.busy":"2024-05-17T07:57:37.536284Z","iopub.execute_input":"2024-05-17T07:57:37.536531Z","iopub.status.idle":"2024-05-17T07:57:49.480110Z","shell.execute_reply.started":"2024-05-17T07:57:37.536510Z","shell.execute_reply":"2024-05-17T07:57:49.479240Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"[';fusharun.' ';bhulthapana.' ';vhayaki.']\n[';फुशारुन.' ';भूलथापाना.' ';व्हायकी.']\ntorch.Size([51200, 30])\ntorch.Size([51200, 30])\ntorch.Size([51200, 30])\n[';garvyabarobarach.' ';reo.' ';sangrahalaye.']\n[';गारव्याबरोबरच.' ';रियो.' ';संग्रहालये.']\ntorch.Size([4096, 30])\ntorch.Size([4096, 30])\ntorch.Size([4096, 30])\n[';heetler.' ';kshama.' ';jinkta.']\n[';हिटलर.' ';क्षमा.' ';जिंकता.']\ntorch.Size([4096, 30])\ntorch.Size([4096, 30])\ntorch.Size([4096, 30])\n\n\nnum of rows: 51200\nnum of columns: 2\ntensor([65, 37, 54, 46, 51, 42, 54, 35, 66,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n","output_type":"stream"}]},{"cell_type":"code","source":"#Used later for reading the words\n\nfrom torch import tensor\ndef reverse_tokenize(data):\n  data=[int(i) for i in data]\n  predicted_seq = [reverse_marathi_token_map[idx] for idx in data]\n  predicted_seq=''.join(predicted_seq)\n  return predicted_seq\n  ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w61HFE3F2aol","outputId":"3bec134a-d2b0-43cc-968b-8109d11a3919","execution":{"iopub.status.busy":"2024-05-17T07:57:49.481360Z","iopub.execute_input":"2024-05-17T07:57:49.481668Z","iopub.status.idle":"2024-05-17T07:57:49.486826Z","shell.execute_reply.started":"2024-05-17T07:57:49.481642Z","shell.execute_reply":"2024-05-17T07:57:49.485845Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"#DATA LOADER","metadata":{"id":"9wDRhKZ4-LHs"}},{"cell_type":"code","source":"\nimport torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nBATCH_SIZE = 512\n\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\n\n#WE ARE NOT USING GENERATE BATCH FUNCTION\ndef generate_batch(data_batch):\n  ma_batch, en_batch = [], []\n  for (en_item,ma_item) in data_batch:\n    #ma_batch.append(torch.cat([torch.tensor(mar_token_map[BOS_IDX]), ma_item, torch.tensor(mar_token_map[EOS_IDX])], dim=0))\n    #en_batch.append(torch.cat([torch.tensor(eng_token_map[BOS_IDX]), en_item, torch.tensor(eng_token_map[EOS_IDX])], dim=0))\n    #print(ma_item[0])\n    ma_batch=torch.tensor(ma_item,dtype=torch.int64)\n    en_batch=torch.tensor(en_item,dtype=torch.int64)\n  \n  return  en_batch.to(device),ma_batch.to(device)\n\n\n\ntrain_iter = DataLoader(train_process, batch_size=BATCH_SIZE,\n                        shuffle=False)\nvalid_iter = DataLoader(val_process, batch_size=BATCH_SIZE,\n                        shuffle=False)\ntest_iter = DataLoader(test_process, batch_size=BATCH_SIZE,\n                       shuffle=False)\nprint(len(train_iter))\nprint(len(test_iter))\nprint(len(valid_iter))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p1SvwaaN-Lj9","outputId":"29549e79-d2c1-4489-912a-5b2d7b3f8856","execution":{"iopub.status.busy":"2024-05-17T07:57:49.488355Z","iopub.execute_input":"2024-05-17T07:57:49.489124Z","iopub.status.idle":"2024-05-17T07:57:49.579730Z","shell.execute_reply.started":"2024-05-17T07:57:49.489091Z","shell.execute_reply":"2024-05-17T07:57:49.578915Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"100\n8\n8\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport string\nimport random\nfrom collections import Counter\n# Set random seed for reproducibility\nSEED = 1234\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\nimport os\n#os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n#os.environ['TORCH_USE_CUDA_DSA'] = '1'\n","metadata":{"id":"547j1j6T2bEu","execution":{"iopub.status.busy":"2024-05-17T07:57:49.580789Z","iopub.execute_input":"2024-05-17T07:57:49.581021Z","iopub.status.idle":"2024-05-17T07:57:49.586855Z","shell.execute_reply.started":"2024-05-17T07:57:49.581000Z","shell.execute_reply":"2024-05-17T07:57:49.586002Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade wandb\n!wandb login f6b40fc0bcc13c1d6117718b14ef1ddd1c68a700 #my API key for wandb login \nimport wandb","metadata":{"id":"rnhhFib9Vrl_","execution":{"iopub.status.busy":"2024-05-17T07:57:49.587837Z","iopub.execute_input":"2024-05-17T07:57:49.588110Z","iopub.status.idle":"2024-05-17T07:58:04.894583Z","shell.execute_reply.started":"2024-05-17T07:57:49.588086Z","shell.execute_reply":"2024-05-17T07:58:04.893459Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.17.0)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (4.2.0)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.45.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#ATTENTION MODEL","metadata":{"id":"P0GVbIJUNEds"}},{"cell_type":"code","source":"import random\nfrom typing import Tuple\n\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass Encoder(nn.Module):\n    \n    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n        super(Encoder, self).__init__()\n        l = [input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout]\n        self.input_dim = l[0]\n        self.emb_dim = l[1]\n        self.enc_hid_dim = l[2]\n        self.dec_hid_dim = l[3]\n        self.dropout = l[4]\n\n        self.embedding = nn.Embedding(l[0], l[1])\n        bidir_bool = True   # Always True\n        # GRU \n        self.rnn = nn.GRU(l[1], l[2], bidirectional=bidir_bool)\n        temp = enc_hid_dim * 2\n        self.fc = nn.Linear(temp, l[3])\n        self.dropout = nn.Dropout(l[4])\n\n    \n\n    \n    def forward(self, src: Tensor) -> Tuple[Tensor]:\n        # Transpose the source tensor\n        src_transposed = src.transpose(0, 1)\n\n        # Embed the transposed source tensor with dropout\n        embedded = self.dropout(self.embedding(src_transposed))\n\n        # Pass the embedded tensor through the RNN\n        outputs, hidden = self.rnn(embedded)\n\n        # Concatenate and apply tanh activation to the last two layers of hidden states\n        last_hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n        hidden_combined = torch.tanh(self.fc(last_hidden))\n\n        return outputs, hidden_combined\n\nclass Attention(nn.Module):\n    \n\n    def __init__(self, enc_hid_dim: int, dec_hid_dim: int, attn_dim: int):\n        super(Attention, self).__init__()\n        hid_l = [enc_hid_dim, dec_hid_dim]\n        self.enc_hid_dim = hid_l[0]\n        self.dec_hid_dim = hid_l[1]\n        enc_hid_dim_double = 2 * enc_hid_dim\n        temp = enc_hid_dim_double + dec_hid_dim\n        self.attn_in = temp\n\n        self.attn = nn.Linear(self.attn_in, attn_dim)\n        \n        \n    def forward(self, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tensor:\n        src_len = encoder_outputs.size(0)\n        \n        decoder_hidden_expanded = decoder_hidden.unsqueeze(1)\n\n        repeated_decoder_hidden = decoder_hidden_expanded.repeat(1, src_len, 1)\n\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n\n        concat_hidden_outputs = torch.cat((repeated_decoder_hidden, encoder_outputs), dim=2)\n        linear_transform = self.attn(concat_hidden_outputs)\n        energy = torch.tanh(linear_transform)\n        attention = torch.sum(energy, dim=2)\n\n        attention_weights = F.softmax(attention, dim=1)\n\n        return attention_weights\n    \n    \nclass Decoder(nn.Module):\n    \n    def __init__(self, output_dim: int, emb_dim: int, enc_hid_dim: int,\n                 dec_hid_dim: int, dropout: float, attention: nn.Module):\n        super().__init__()\n        cons_l = [output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention]\n        self.output_dim = cons_l[0]\n        self.emb_dim = cons_l[1]\n        self.enc_hid_dim =cons_l[2] \n        self.dec_hid_dim = cons_l[3]\n        self.dropout_p = cons_l[4]\n        self.attention = cons_l[5]\n\n        self.embedding = nn.Embedding(cons_l[0], cons_l[1])\n        self.rnn_input_size = cons_l[2] * 2 + cons_l[1]\n        self.rnn = nn.GRU(self.rnn_input_size, cons_l[3])\n        self.out_input_size = self.attention.attn_in + cons_l[1]\n        self.out = nn.Linear(self.out_input_size, cons_l[0])\n        self.dropout = nn.Dropout(cons_l[4])\n    \n\n\n    def _weighted_encoder_rep(self, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tensor:\n        # Calculate attention weights\n        attention_weights = self.attention(decoder_hidden, encoder_outputs)\n        attention_weights = attention_weights.unsqueeze(1)  # Add a dimension for batch multiplication\n\n        # Transpose encoder outputs for correct alignment\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n\n        # Calculate weighted encoder representation\n        weighted_encoder_rep = torch.bmm(attention_weights, encoder_outputs)\n        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)  # Transpose back for output\n\n        return weighted_encoder_rep\n    \n    def forward(self, input: Tensor, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tuple[Tensor]:\n        # Add a batch dimension to input\n        input = input.unsqueeze(0)\n\n        # Transpose input tensor\n        input = input.permute(1, 0)\n\n        # Apply embedding to the input tensor\n        embedded_input = self.embedding(input)\n\n        # Apply dropout to the embedded input tensor\n        embedded = self.dropout(embedded_input)\n\n        # Calculate the weighted encoder representation\n        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden, encoder_outputs)\n\n        # Transpose embedded tensor for concatenation\n        embedded = embedded.permute(1, 0, 2)\n\n        # Concatenate embedded tensor and weighted encoder representation\n        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim=2)\n\n        # Pass through the GRU layer\n        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n\n        # Squeeze the tensors to remove batch dimension\n        embedded = embedded.squeeze(0)\n        output = output.squeeze(0)\n        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n\n        # Concatenate output, weighted encoder representation, and embedded tensor\n        concatenated_output = torch.cat((output, weighted_encoder_rep, embedded), dim=1)\n\n        # Pass through the output layer\n        final_output = self.out(concatenated_output)\n\n        return final_output, decoder_hidden.squeeze(0)\n    \n\n\nclass Seq2Seq(nn.Module):\n    def __init__(self,encoder: nn.Module,decoder: nn.Module,device: torch.device):\n        super().__init__()\n\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self,src: Tensor,trg: Tensor,teacher_forcing_ratio: float = 0.5) -> Tensor:\n\n        batch_size = src.shape[0]\n        max_len = trg.shape[1]\n        trg_vocab_size = self.decoder.output_dim\n        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n        #ATTENTION HEAT MAP METHOD\n        #attentions = torch.zeros(max_len, batch_size, src.shape[1]).to(self.device)\n\n        encoder_outputs, hidden = self.encoder(src)\n\n        # first input to the decoder is the <sos> token\n        trg = trg.permute(1,0)\n        output = trg[0,:]\n        \n        t = 1\n        while t < max_len:\n            output, hidden = self.decoder(output, hidden, encoder_outputs)\n            outputs[t] = output\n            if random.random() < teacher_forcing_ratio:\n                output = trg[t]\n            else:\n                output = output.max(1)[1]\n            t += 1\n\n        return outputs\n\n\n\ndef init_weights(m: nn.Module):\n    params = iter(m.named_parameters())\n    try:\n        while True:\n            name, param = next(params)\n            if 'weight' in name:\n                nn.init.normal_(param.data, mean=0, std=0.01)\n            else:\n                nn.init.constant_(param.data, 0)\n    except StopIteration:\n        pass\n\n\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4HnpQyXnNBh0","outputId":"b6464bca-120d-40d2-be6f-12a87adb9169","execution":{"iopub.status.busy":"2024-05-17T07:58:04.896645Z","iopub.execute_input":"2024-05-17T07:58:04.897329Z","iopub.status.idle":"2024-05-17T07:58:04.936889Z","shell.execute_reply.started":"2024-05-17T07:58:04.897289Z","shell.execute_reply":"2024-05-17T07:58:04.936110Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# ATTENTION WANDB IMPLEMENTATION","metadata":{"id":"QP2vxY5EIul0"}},{"cell_type":"code","source":"import math\nimport time\n\nsweep_config = {\n            \n            'method': 'random',\n            'metric': { 'goal': 'maximize','name': 'Accuracy'},\n            'parameters': \n                {\n                    'num_epochs': {'values': [10]},\n                    'cell_type': {'values': ['RNN', 'LSTM', 'GRU']},\n                    'emb_dim': {'values': [128, 256, 512]},\n                    'hidden_layer_dim': {'values': [128, 256, 512]},\n                    'num_layers': {'values': [1, 2, 3]},\n                    'dropout': {'values': [0.3, 0.5, 0.7]},\n                    'attn_dim':{'values':[64,128,256]},\n                    'optimizer' : {'values' : ['adam', 'rmsprop', 'adagrad']},\n                    'learning_rate': {'values': [0.001, 0.005, 0.01, 0.1]},\n                    'batch_size': {'values': [32, 64]},\n                    'teacher_fr' : {'values': [0.3, 0.5, 0.7]},\n                    'length_penalty' : {'values': [0.4, 0.5, 0.6]},\n                    'bi_dir' : {'values': [True, False]},\n                    'beam_width': {'values': [1, 2, 3, 4, 5]}\n                }\n            }\nsweep_id = wandb.sweep(sweep_config, project=\"CS6910-Assignment3\")\ndef train_model(model, train_iter, optimizer, criterion, device, CLIP):\n    model.train()\n    train_epoch_loss = 0\n    train_epoch_acc = 0\n\n    for _, (src, trg) in enumerate(train_iter):\n        optimizer.zero_grad()\n        src = src.to(device)\n        trg = trg.to(device)\n\n        output = model(src, trg)\n\n        output = output[1:].view(-1, output.shape[-1])\n        trg = trg.permute(1, 0)\n        trg = torch.reshape(trg[1:], (-1,))\n        loss = criterion(output, trg)\n\n        preds = torch.argmax(output, dim=1)\n        non_pad_elements = (trg != 0).nonzero(as_tuple=True)[0]\n        train_correct = preds[non_pad_elements] == trg[non_pad_elements]\n        train_epoch_acc += train_correct.sum().item() / len(non_pad_elements)\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n\n        optimizer.step()\n\n        train_epoch_loss += loss.item()\n\n    train_epoch_loss /= len(train_iter)\n    train_epoch_acc /= len(train_iter)\n\n    return train_epoch_loss, train_epoch_acc\n\ndef evaluate_model(model, valid_iter, criterion, device):\n    model.eval()\n    val_epoch_loss = 0\n    val_epoch_acc = 0\n\n    with torch.no_grad():\n        for _, (src, trg) in enumerate(valid_iter):\n            src, trg = src.to(device), trg.to(device)\n\n            output = model(src, trg, 0)\n\n            output = output[1:].view(-1, output.shape[-1])\n            trg = trg.permute(1, 0)\n            trg = torch.reshape(trg[1:], (-1,))\n\n            loss = criterion(output, trg)\n\n            val_epoch_loss += loss.item()\n\n            preds = torch.argmax(output, dim=1)\n            non_pad_elements = (trg != 0).nonzero(as_tuple=True)[0]\n            val_correct = preds[non_pad_elements] == trg[non_pad_elements]\n            val_epoch_acc += val_correct.sum().item() / len(non_pad_elements)\n\n    val_epoch_loss /= len(valid_iter)\n    val_epoch_acc /= len(valid_iter)\n\n    return val_epoch_loss, val_epoch_acc\n\ndef run_training_loop(model, train_iter, valid_iter, optimizer, criterion, device, CLIP, N_EPOCHS):\n    for epoch in range(N_EPOCHS):\n        train_epoch_loss, train_epoch_acc = train_model(model, train_iter, optimizer, criterion, device, CLIP)\n        val_epoch_loss, val_epoch_acc = evaluate_model(model, valid_iter, criterion, device)\n\n        print(f'Epoch: {epoch+1:02} | Train accuracy: {train_epoch_acc*100:.3f} | Val accuracy: {val_epoch_acc*100:.3f}')\n        wandb.log({\"train_loss\": train_epoch_loss, \"train_accuracy\": train_epoch_acc, \"val_loss\": val_epoch_loss, \"val_accuracy\": val_epoch_acc})\n\n        if epoch == N_EPOCHS - 1:\n            torch.cuda.empty_cache()\n\n\ndef sweep_train():\n    # Default values for hyper-parameters we're going to sweep over\n    config_defaults = {'emb_dim':256,'hidden_layer_dim':256,'attn_dim':128,'dropout':0.6,}\n    # Initialize a new wandb run\n    wandb.init(project='CS6910-Assignment3',config=config_defaults)\n    wandb.run.name = (\n        'Q2_c:' + str(wandb.config.cell_type) +\n        '_e' + str(wandb.config.num_epochs) +\n        '_es:' + str(wandb.config.emb_dim) +\n        '_hs:' + str(wandb.config.hidden_layer_dim) +\n        '_nle:' + str(wandb.config.num_layers) +\n        '_nld:' + str(wandb.config.num_layers) +\n        '_o:' + str(wandb.config.optimizer) +\n        '_lr:' + str(wandb.config.learning_rate) +\n        '_bs:' + str(wandb.config.batch_size) +\n        '_tf:' + str(wandb.config.teacher_fr) +\n        '_lp:' + str(wandb.config.length_penalty) +\n        '_b:' + str(wandb.config.bi_dir) +\n        '_bw:' + str(wandb.config.beam_width)\n    ) \n\n    emb_dim = wandb.config.emb_dim\n    hidden_layer_dim = wandb.config.hidden_layer_dim\n    attn_dim = wandb.config.attn_dim\n    dropout = wandb.config.dropout\n\n    token_map_list_length = [len(eng_token_map), len(mar_token_map)]  \n    INPUT_DIM = token_map_list_length[0]\n    OUTPUT_DIM = token_map_list_length[1]\n    enc_param = [INPUT_DIM, emb_dim, hidden_layer_dim, hidden_layer_dim, dropout]\n    enc = Encoder(enc_param[0], enc_param[1],enc_param[2],enc_param[3],enc_param[4])\n    att_param = [hidden_layer_dim, hidden_layer_dim, attn_dim]\n    attn = Attention(att_param[0], att_param[1], att_param[2])\n    dec_param = [OUTPUT_DIM, emb_dim, hidden_layer_dim, hidden_layer_dim, dropout, attn]\n    dec = Decoder(dec_param[0],dec_param[1],dec_param[2],dec_param[3],dec_param[4],dec_param[5])\n\n    model = Seq2Seq(enc, dec, device).to(device)\n    model.apply(init_weights)\n    opti = wandb.config.optimizer\n    if opti == \"adam\":\n        optimizer = optim.Adam(model.parameters())\n    if opti == \"rmsprop\":\n        optimizer = optim.RMSprop(model.parameters())\n    if opti == \"adagrad\":\n        optimizer = optim.Adagrad(model.parameters())\n    else:\n        optimizer = optim.Adam(model.parameters())  \n\n    criterion = nn.CrossEntropyLoss()\n\n    model.train()\n    train_epoch_loss = 0\n    val_epoch_loss,val_epoch_acc = 0,0\n    train_epoch_acc = 0\n    \n\n    N_EPOCHS = wandb.config.num_epochs\n    CLIP = 1\n\n\n    # Example usage:\n    run_training_loop(model, train_iter, valid_iter, optimizer, criterion, device, CLIP, N_EPOCHS)\n\n#RUNNING THE SWEEP\nwandb.agent(sweep_id, function=sweep_train, count=120)","metadata":{"id":"KwA5nEgVItcQ","execution":{"iopub.status.busy":"2024-05-17T07:58:04.940414Z","iopub.execute_input":"2024-05-17T07:58:04.940763Z","iopub.status.idle":"2024-05-17T07:59:02.564766Z","shell.execute_reply.started":"2024-05-17T07:58:04.940739Z","shell.execute_reply":"2024-05-17T07:59:02.563632Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Create sweep with ID: 5gpauuz3\nSweep URL: https://wandb.ai/cs23m074yash/CS6910-Assignment3/sweeps/5gpauuz3\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 67jcquly with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattn_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbi_dir: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_dim: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlength_penalty: 0.6\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_fr: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23m074\u001b[0m (\u001b[33mcs23m074yash\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240517_075808-67jcquly</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs23m074yash/CS6910-Assignment3/runs/67jcquly' target=\"_blank\">fresh-sweep-1</a></strong> to <a href='https://wandb.ai/cs23m074yash/CS6910-Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs23m074yash/CS6910-Assignment3/sweeps/5gpauuz3' target=\"_blank\">https://wandb.ai/cs23m074yash/CS6910-Assignment3/sweeps/5gpauuz3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs23m074yash/CS6910-Assignment3' target=\"_blank\">https://wandb.ai/cs23m074yash/CS6910-Assignment3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs23m074yash/CS6910-Assignment3/sweeps/5gpauuz3' target=\"_blank\">https://wandb.ai/cs23m074yash/CS6910-Assignment3/sweeps/5gpauuz3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs23m074yash/CS6910-Assignment3/runs/67jcquly' target=\"_blank\">https://wandb.ai/cs23m074yash/CS6910-Assignment3/runs/67jcquly</a>"},"metadata":{}},{"name":"stdout","text":"Epoch: 01 | Train accuracy: 8.661 | Val accuracy: 7.633\nEpoch: 02 | Train accuracy: 18.487 | Val accuracy: 12.640\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n","output_type":"stream"}]},{"cell_type":"code","source":"config_defaults = {'emb_dim':256,'hidden_layer_dim':256,'attn_dim':128,'dropout':0.6,}\n    \nemb_dim = 256\nhidden_layer_dim = 256\nattn_dim = 128\ndropout = 0.6\n\ntoken_map_list_length = [len(eng_token_map), len(mar_token_map)]  \nINPUT_DIM = token_map_list_length[0]\nOUTPUT_DIM = token_map_list_length[1]\nenc_param = [INPUT_DIM, emb_dim, hidden_layer_dim, hidden_layer_dim, dropout]\nenc = Encoder(enc_param[0], enc_param[1],enc_param[2],enc_param[3],enc_param[4])\natt_param = [hidden_layer_dim, hidden_layer_dim, attn_dim]\nattn = Attention(att_param[0], att_param[1], att_param[2])\ndec_param = [OUTPUT_DIM, emb_dim, hidden_layer_dim, hidden_layer_dim, dropout, attn]\ndec = Decoder(dec_param[0],dec_param[1],dec_param[2],dec_param[3],dec_param[4],dec_param[5])\n\nmodel = Seq2Seq(enc, dec, device).to(device)\nmodel.apply(init_weights)\nopti = \"adam\"\nif opti == \"adam\":\n    optimizer = optim.Adam(model.parameters())\nif opti == \"rmsprop\":\n    optimizer = optim.RMSprop(model.parameters())\nif opti == \"adagrad\":\n    optimizer = optim.Adagrad(model.parameters())\nelse:\n    optimizer = optim.Adam(model.parameters())  \n\ncriterion = nn.CrossEntropyLoss()\n\nmodel.train()\n\ntrain_epoch_loss = 0\ntrain_epoch_acc = 0\nval_epoch_loss = 0\nval_epoch_acc = 0\n\nN_EPOCHS = 10\nCLIP = 1\nrun_training_loop(model, train_iter, valid_iter, optimizer, criterion, device, CLIP, N_EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T07:59:18.000277Z","iopub.execute_input":"2024-05-17T07:59:18.000672Z","iopub.status.idle":"2024-05-17T08:02:39.211972Z","shell.execute_reply.started":"2024-05-17T07:59:18.000644Z","shell.execute_reply":"2024-05-17T08:02:39.210931Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render progress bar, see the user log for details\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Problem finishing run\nException in thread Thread-6 (_run_job):\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_6751/750440767.py\", line 164, in sweep_train\n  File \"/tmp/ipykernel_6751/750440767.py\", line 95, in run_training_loop\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/redirect.py\", line 645, in write\n    self._old_write(data)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/iostream.py\", line 662, in write\n    self._schedule_flush()\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/iostream.py\", line 559, in _schedule_flush\n    self.pub_thread.schedule(_schedule_in_thread)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/iostream.py\", line 266, in schedule\n    self._event_pipe.send(b\"\")\n  File \"/opt/conda/lib/python3.10/site-packages/zmq/sugar/socket.py\", line 696, in send\n    return super().send(data, flags=flags, copy=copy, track=track)\n  File \"zmq/backend/cython/socket.pyx\", line 742, in zmq.backend.cython.socket.Socket.send\n  File \"zmq/backend/cython/socket.pyx\", line 783, in zmq.backend.cython.socket.Socket.send\n  File \"zmq/backend/cython/socket.pyx\", line 138, in zmq.backend.cython.socket._check_closed\nzmq.error.ZMQError: Socket operation on non-socket\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/ipython.py\", line 113, in update\n    self._progress.value = value\n  File \"/opt/conda/lib/python3.10/site-packages/traitlets/traitlets.py\", line 732, in __set__\n    self.set(obj, value)\n  File \"/opt/conda/lib/python3.10/site-packages/traitlets/traitlets.py\", line 721, in set\n    obj._notify_trait(self.name, old_value, new_value)\n  File \"/opt/conda/lib/python3.10/site-packages/traitlets/traitlets.py\", line 1505, in _notify_trait\n    self.notify_change(\n  File \"/opt/conda/lib/python3.10/site-packages/ipywidgets/widgets/widget.py\", line 685, in notify_change\n    self.send_state(key=name)\n  File \"/opt/conda/lib/python3.10/site-packages/ipywidgets/widgets/widget.py\", line 554, in send_state\n    self._send(msg, buffers=buffers)\n  File \"/opt/conda/lib/python3.10/site-packages/ipywidgets/widgets/widget.py\", line 817, in _send\n    self.comm.send(data=msg, buffers=buffers)\n  File \"/opt/conda/lib/python3.10/site-packages/comm/base_comm.py\", line 147, in send\n    self.publish_msg(\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/comm/comm.py\", line 37, in publish_msg\n    self.kernel.session.send(\n  File \"/opt/conda/lib/python3.10/site-packages/jupyter_client/session.py\", line 863, in send\n    stream.send_multipart(to_send, copy=copy)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/iostream.py\", line 345, in send_multipart\n    return self.io_thread.send_multipart(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/iostream.py\", line 275, in send_multipart\n    self.schedule(lambda: self._really_send(*args, **kwargs))\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/iostream.py\", line 266, in schedule\n    self._event_pipe.send(b\"\")\n  File \"/opt/conda/lib/python3.10/site-packages/zmq/sugar/socket.py\", line 696, in send\n    return super().send(data, flags=flags, copy=copy, track=track)\n  File \"zmq/backend/cython/socket.pyx\", line 742, in zmq.backend.cython.socket.Socket.send\n  File \"zmq/backend/cython/socket.pyx\", line 783, in zmq.backend.cython.socket.Socket.send\n  File \"zmq/backend/cython/socket.pyx\", line 138, in zmq.backend.cython.socket._check_closed\nzmq.error.ZMQError: Socket operation on non-socket\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 2352, in _atexit_cleanup\n    self._on_finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 2608, in _on_finish\n    _ = exit_handle.wait(timeout=-1, on_progress=self._on_progress_exit)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py\", line 303, in wait\n    on_progress(progress_handle)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 2587, in _on_progress_exit\n    self._footer_file_pusher_status_info(\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 3807, in _footer_file_pusher_status_info\n    Run._footer_single_run_file_pusher_status_info(\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 3858, in _footer_single_run_file_pusher_status_info\n    printer.progress_update(line, percent_done)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/printer.py\", line 288, in progress_update\n    self._progress.update(percent_done, text)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/ipython.py\", line 121, in update\n    wandb.termwarn(\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/errors/term.py\", line 53, in termwarn\n    _log(\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/errors/term.py\", line 103, in _log\n    click.echo(line, file=sys.stderr, nl=newline)\n  File \"/opt/conda/lib/python3.10/site-packages/click/utils.py\", line 318, in echo\n    file.write(out)  # type: ignore\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/iostream.py\", line 662, in write\n    self._schedule_flush()\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/iostream.py\", line 559, in _schedule_flush\n    self.pub_thread.schedule(_schedule_in_thread)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/iostream.py\", line 266, in schedule\n    self._event_pipe.send(b\"\")\n  File \"/opt/conda/lib/python3.10/site-packages/zmq/sugar/socket.py\", line 696, in send\n    return super().send(data, flags=flags, copy=copy, track=track)\n  File \"zmq/backend/cython/socket.pyx\", line 742, in zmq.backend.cython.socket.Socket.send\n  File \"zmq/backend/cython/socket.pyx\", line 783, in zmq.backend.cython.socket.Socket.send\n  File \"zmq/backend/cython/socket.pyx\", line 138, in zmq.backend.cython.socket._check_closed\nzmq.error.ZMQError: Socket operation on non-socket\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 312, in _run_job\n    wandb.finish(exit_code=1)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 4247, in finish\n    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 449, in wrapper\n    return func(self, *args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 390, in wrapper\n    return func(self, *args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 2094, in finish\n    return self._finish(exit_code, quiet)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 2109, in _finish\n    self._atexit_cleanup(exit_code=exit_code)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 2365, in _atexit_cleanup\n    wandb.termerror(\"Problem finishing run\")\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/errors/term.py\", line 64, in termerror\n    _log(\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/errors/term.py\", line 103, in _log\n    click.echo(line, file=sys.stderr, nl=newline)\n  File \"/opt/conda/lib/python3.10/site-packages/click/utils.py\", line 319, in echo\n    file.flush()\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/iostream.py\", line 573, in flush\n    self.pub_thread.schedule(self._flush)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/iostream.py\", line 266, in schedule\n    self._event_pipe.send(b\"\")\n  File \"/opt/conda/lib/python3.10/site-packages/zmq/sugar/socket.py\", line 696, in send\n    return super().send(data, flags=flags, copy=copy, track=track)\n  File \"zmq/backend/cython/socket.pyx\", line 742, in zmq.backend.cython.socket.Socket.send\n  File \"zmq/backend/cython/socket.pyx\", line 783, in zmq.backend.cython.socket.Socket.send\n  File \"zmq/backend/cython/socket.pyx\", line 138, in zmq.backend.cython.socket._check_closed\nzmq.error.ZMQError: Socket operation on non-socket\nException in threading.excepthook:\nException ignored in thread started by: <bound method Thread._bootstrap of <Thread(Thread-6 (_run_job), stopped 137804372125440)>>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1018, in _bootstrap_inner\n    self._invoke_excepthook(self)\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1336, in invoke_excepthook\n    local_print(\"Exception in threading.excepthook:\",\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/iostream.py\", line 573, in flush\n    self.pub_thread.schedule(self._flush)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/iostream.py\", line 266, in schedule\n    self._event_pipe.send(b\"\")\n  File \"/opt/conda/lib/python3.10/site-packages/zmq/sugar/socket.py\", line 696, in send\n    return super().send(data, flags=flags, copy=copy, track=track)\n  File \"zmq/backend/cython/socket.pyx\", line 742, in zmq.backend.cython.socket.Socket.send\n  File \"zmq/backend/cython/socket.pyx\", line 783, in zmq.backend.cython.socket.Socket.send\n  File \"zmq/backend/cython/socket.pyx\", line 138, in zmq.backend.cython.socket._check_closed\nzmq.error.ZMQError: Socket operation on non-socket\nException ignored in sys.unraisablehook: <built-in function unraisablehook>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/iostream.py\", line 573, in flush\n    self.pub_thread.schedule(self._flush)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/iostream.py\", line 266, in schedule\n    self._event_pipe.send(b\"\")\n  File \"/opt/conda/lib/python3.10/site-packages/zmq/sugar/socket.py\", line 696, in send\n    return super().send(data, flags=flags, copy=copy, track=track)\n  File \"zmq/backend/cython/socket.pyx\", line 742, in zmq.backend.cython.socket.Socket.send\n  File \"zmq/backend/cython/socket.pyx\", line 783, in zmq.backend.cython.socket.Socket.send\n  File \"zmq/backend/cython/socket.pyx\", line 138, in zmq.backend.cython.socket._check_closed\nzmq.error.ZMQError: Socket operation on non-socket\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 04 | Train accuracy: 20.694 | Val accuracy: 14.700Epoch: 01 | Train accuracy: 7.758 | Val accuracy: 11.360\nEpoch: 02 | Train accuracy: 18.093 | Val accuracy: 13.222\nEpoch: 03 | Train accuracy: 20.803 | Val accuracy: 13.986\nEpoch: 04 | Train accuracy: 21.979 | Val accuracy: 16.370\nEpoch: 05 | Train accuracy: 23.607 | Val accuracy: 19.237\nEpoch: 06 | Train accuracy: 26.872 | Val accuracy: 21.840\nEpoch: 07 | Train accuracy: 29.197 | Val accuracy: 25.246\nEpoch: 08 | Train accuracy: 35.041 | Val accuracy: 31.103\nEpoch: 09 | Train accuracy: 43.542 | Val accuracy: 42.867\nEpoch: 10 | Train accuracy: 56.787 | Val accuracy: 52.373\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Vanilla Seq2Seq\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\n\nimport math\nimport time\nimport pandas as pd\nimport numpy as np\nimport random\nfrom typing import Tuple\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport string\nimport random\nfrom collections import Counter\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Load","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef load_data(path):\n    data = pd.read_csv(path, header=None, names=[\"en\", \"ma\", \"\"], skip_blank_lines=True)\n    data = data.dropna(subset=['en', 'ma'])\n    return data[['en', 'ma']]\n\n# Define file paths\nfile_paths = [\n    \"/kaggle/input/marathi-dataset/mar/mar_train.csv\",\n    \"/kaggle/input/marathi-dataset/mar/mar_valid.csv\",\n    \"/kaggle/input/marathi-dataset/mar/mar_test.csv\"\n]\n\n# Load data for train, val, and test\ntrain, val, test = [load_data(path) for path in file_paths]\n\n# Display test data and lengths\n# print(test)\ntest_en = test['en'].tolist()\ntest_ma = test['ma'].tolist()\n# print(test_en)\n# print(test_ma)\nprint(len(train))\nprint(len(test))\nprint(len(val))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unique_tokenize(data):\n    english_tokens = set(''.join(data['en']))\n    marathi_tokens = set(''.join(data['ma']))\n    return sorted(list(marathi_tokens)), sorted(list(english_tokens))\nmarathi_tokens , english_tokens = unique_tokenize(train)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#REWRITE AGAIN\ndef tokenize_map(hindi_tokens , english_tokens):\n    english_token_map = dict([(ch,i+1) for i,ch in enumerate(english_tokens)])\n    marathi_token_map = dict([(ch,i+1) for i,ch in enumerate(marathi_tokens)])\n    reverse_marathi_token_map = dict([(i+1,ch) for i,ch in enumerate(marathi_tokens)])\n    #Adding blank space\n\n    marathi_token_map[\" \"] = 0\n    english_token_map[\" \"] = 0\n    #addin BOS and EOS token \n    marathi_token_map[';']=65\n    marathi_token_map['.']=66\n    english_token_map[';']=27\n    english_token_map['.']=28\n    marathi_token_map['<unk>']=64\n    \n    reverse_marathi_token_map[64]='<unk>'\n    reverse_marathi_token_map[65]=';'\n    reverse_marathi_token_map[66]='.'\n    reverse_marathi_token_map[0]=''\n\n    return marathi_token_map, english_token_map,reverse_marathi_token_map\n\nmar_token_map, eng_token_map,reverse_marathi_token_map = tokenize_map(marathi_tokens , english_tokens)\nprint(mar_token_map)\nprint(reverse_marathi_token_map)\nprint(eng_token_map)\n\nprint('mar:',len(mar_token_map))\nprint('eng:',len(eng_token_map))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nx = test['en'].values\ny = test['ma'].values\n\nmax_eng_len = 2+max([len(i) for i in x]) \nmax_mar_len = 2+max([len(i) for i in y])\nprint(max_eng_len,max_mar_len)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# One hot Encode","metadata":{}},{"cell_type":"code","source":"\nimport torch\nunknown_token=64\ndef process(data):\n    x, y = data['en'].values, data['ma'].values\n\n    # Add start and end tokens to sentences\n    x = [';' + sentence + '.' for sentence in x]\n    y = [';' + sentence + '.' for sentence in y]\n    a = torch.zeros((len(x), max_eng_len), dtype=torch.int64)\n    b = torch.zeros((len(y), max_eng_len), dtype=torch.int64)\n\n    # Process data\n    for i, (xx, yy) in enumerate(zip(x, y)):\n        for j, ch in enumerate(xx):\n            a[i, j] = eng_token_map.get(ch, unknown_token)\n\n        for j, ch in enumerate(yy):\n            b[i, j] = mar_token_map.get(ch, unknown_token)\n\n\n    # Create list of tuples (input tensor, target tensor)\n    data = [(a[i], b[i]) for i in range(len(x))]\n\n    return data\n\n\ntrain_process=process(train)\nval_process=process(val)\ntest_process=process(test)#print(train_process.shape)\nprint('\\n')\nprint('num of rows:',len(train_process))\nprint('num of columns:',len(train_process[0]))\nprint(train_process[0][1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef reverse_tokenize(data):\n    data = list(map(int, data))\n    predicted_seq = ''.join(map(lambda idx: reverse_marathi_token_map[idx], data))\n    return predicted_seq","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nBATCH_SIZE = 16\n\n\ndef dataL(P, BATCH_SIZE, shuffle_bool):\n    iter = DataLoader(P, batch_size=BATCH_SIZE, shuffle=shuffle_bool)\n\n    return iter\n\n\n\ntrain_iter = dataL(train_process, BATCH_SIZE, False)\nvalid_iter = dataL(val_process, BATCH_SIZE, False)\ntest_iter = dataL(test_process, BATCH_SIZE, False)\n\nprint(len(train_iter))\nprint(len(test_iter))\nprint(len(valid_iter))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Wandb login","metadata":{}},{"cell_type":"code","source":"\n!wandb login f6b40fc0bcc13c1d6117718b14ef1ddd1c68a700 #my API key for wandb login \nimport wandb","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vanilla encoder and decoder","metadata":{}},{"cell_type":"code","source":"# Define the vocabulary of English and Devanagari characters\n\n# Define the maximum sequence lengths for input and output sequences\nMAX_LEN_EN = 30\nMAX_LEN_MA = 30\n\n# Define the start and end of sequence tokens\nBOS_token = 28\nEOS_token = 29\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers=1 , cell_type=\"lstm\", p=0.5):\n        super().__init__()\n        eL = [input_size, embedding_size, hidden_size, num_layers , cell_type, p]\n        self.embedding = nn.Embedding(eL[0],eL[1])\n        self.hidden_size = eL[2]\n        self.num_layers = eL[3]\n        self.cell_type = eL[4]\n        self.dropout=nn.Dropout(eL[5])\n        prob = p\n        if cell_type.upper() == \"GRU\":\n            self.rnn = nn.GRU(input_size, hidden_size, num_layers,dropout=prob)\n        if cell_type.upper() == \"LSTM\":\n            self.rnn = nn.LSTM(input_size, hidden_size, num_layers,dropout=prob)\n        if cell_type.upper() != \"RNN\":\n            self.rnn = nn.GRU(input_size, hidden_size, num_layers,dropout=prob)\n        else:\n            self.rnn = nn.RNN(input_size, hidden_size, num_layers,dropout=prob)\n    def forward(self, x):\n        \n        x = x.permute(1,0)\n        pre_embedding = self.embedding(x)\n        embedding=self.dropout(pre_embedding)\n        if self.cell_type != \"lstm\":\n            hidden, cell = self.rnn(embedding)\n        elif self.cell_type == \"lstm\":\n            outputs,(hidden,cell)=self.rnn(embedding)\n        return hidden, cell\n\n\nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers=1, cell_type=\"lstm\",p=0.5):\n        super().__init__()\n        dL = [input_size, embedding_size, hidden_size, output_size, num_layers, cell_type,p]\n        self.embedding=nn.Embedding(dL[0], dL[1])\n        self.hidden_size = dL[2]\n        self.cell_type=dL[5]\n        self.num_layers = dL[4]\n        self.dropout=nn.Dropout(dL[6])\n\n        self.output_size = dL[3]\n        if cell_type.upper() == \"GRU\":\n            self.rnn = nn.GRU(output_size, hidden_size, num_layers)\n        if cell_type.lower() != \"lstm\":\n            self.rnn = nn.RNN(output_size, hidden_size, num_layers)\n        if cell_type.upper() == \"RNN\":\n            self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers,dropout=p)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, hidden,cell):\n       \n       x = x.unsqueeze(0)\n       pre_embeddingD = self.embedding(x) \n       embedding= self.dropout(pre_embeddingD)\n       \n       if self.cell_type!=\"lstm\":\n         outputs, hidden = self.rnn(embedding, hidden)\n       else:\n         outputs,(hidden,cell) = self.rnn(embedding, (hidden,cell))\n       \n\n       predictions=self.fc(outputs)\n       \n       tempD = 0\n       predictions = predictions.squeeze(tempD)\n       \n       if self.cell_type != \"lstm\":\n         return predictions, hidden\n       else:\n         return predictions, hidden, cell\n\nclass Seq2Seq(nn.Module):\n    def __init__(self,encoder,decoder,cell_type=\"lstm\"):\n        sL = [encoder,decoder,cell_type]\n        super().__init__()\n        self.encoder = sL[0]\n        self.decoder = sL[1]\n        self.cell_type = sL[2]\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        sFL = [source, target, teacher_force_ratio]\n        batch_size = sFL[0].shape[0]\n        \n        target_len = sFL[1].shape[1]\n        target_vocab_size = self.decoder.output_size\n\n        outputs = torch.zeros(target_len,batch_size,target_vocab_size).to(device)\n        \n        cell, hidden = self.encoder(sFL[0])\n        \n        target = sFL[1].permute(1,0)\n        x = target[0,:]\n        t = 1\n        while t < target_len:\n          if self.cell_type!=\"lstm\":\n            output, hidden = self.decoder(x,hidden,cell)\n          else:\n           output,hidden,cell =self.decoder(x,hidden,cell)\n\n          outputs[t] = output\n          \n          best_guess = output.argmax(1)\n          if random.random() >= teacher_force_ratio:\n            x = best_guess  \n          else:\n            x = target[t]\n          t += 1\n        return outputs\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndevice=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_epochs=10\nlearning_rate=0.001\nlen_token_map = [len(eng_token_map), len(mar_token_map)]\ninput_size_encoder = len_token_map[0]\ninput_size_decoder= len_token_map[1]\noutput_size= len_token_map[1]\nmax_size_E_D = [29, 67]\nencoder_embedding_size = max_size_E_D[0]\ndecoder_embedding_size = max_size_E_D[1]\n\ncell_type=\"gru\"\nhidden_size=256\nnum_layers=4\nenc_dropout=0.2\ndec_dropout=0.2\ntrain_L = [input_size_encoder, encoder_embedding_size, hidden_size, num_layers , cell_type]\nencoder_net=Encoder(len_token_map[0],max_size_E_D[0],train_L[2],train_L[3],train_L[4],p=enc_dropout).to(device)\ndecoder_net=Decoder(len_token_map[1],max_size_E_D[1],train_L[2],len_token_map[1],train_L[3],train_L[4],p=dec_dropout).to(device)\nmodel_L = [encoder_net,decoder_net,cell_type]\nmodel = Seq2Seq(model_L[0],model_L[1],model_L[2]).to(device)\ncriterion = nn.CrossEntropyLoss()\n\n\noptimizer = optim.Adam(model.parameters(),lr=learning_rate)\ndef train(model: nn.Module, iterator: torch.utils.data.DataLoader, optimizer: optim.Optimizer, criterion: nn.Module, clip: float):\n\n\n    epoch_loss, epoch_acc = 0, 0\n    model.train()\n\n\n    iterator_iter = iter(iterator)\n    index = 0\n    while True:\n        try:\n            src, trg = next(iterator_iter)\n            optimizer.zero_grad()\n            src = src.to(device)\n            trg = trg.to(device)\n            output = model(src, trg)\n\n            output = output[1:].view(-1, output.shape[-1])\n            trg = trg.permute(1,0)\n            trg = torch.reshape(trg[1:], (-1,))\n            loss = criterion(output, trg)\n\n            # calculate accuracy\n            preds = torch.argmax(output, dim=1)\n\n            non_zero_elements = (trg != 0)\n            non_zero_indices = non_zero_elements.nonzero(as_tuple=True)\n            non_pad_elements = non_zero_indices[0]\n\n\n            preds_non_pad = preds[non_pad_elements]\n            trg_non_pad = trg[non_pad_elements]\n            correct = preds_non_pad == trg_non_pad\n\n            correct_sum = correct.sum().item()\n            non_pad_length = len(non_pad_elements)\n            accuracy = correct_sum / non_pad_length\n            epoch_acc = epoch_acc + accuracy\n\n            loss.backward()\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n\n            optimizer.step()\n\n            epoch_loss = epoch_loss + loss.item()\n            index += 1\n        except StopIteration:\n            break\n    loss_actual = epoch_loss / len(iterator)\n    accuracy_actual = epoch_acc / len(iterator)\n    return  loss_actual, accuracy_actual\n\n\ndef evaluate(model: nn.Module,iterator: torch.utils.data.DataLoader,criterion: nn.Module):\n    epoch_loss, epoch_acc = 0, 0\n\n    solution=[]\n    model.eval()\n    \n    with torch.no_grad():\n        iterator_iter = iter(iterator)\n        \n        index = 0\n        while True:\n          try:\n            src, trg = next(iterator_iter)\n            src = src.to(device)\n            trg = trg.to(device)\n\n            output = model(src, trg, 0) #turn off teacher forcing\n            output = output[1:].view(-1, output.shape[-1])\n            \n            trg = trg.permute(1,0)\n            trg = torch.reshape(trg[1:], (-1,))\n\n            loss = criterion(output, trg)\n\n            epoch_loss = epoch_loss + loss.item()\n            # calculate accuracy\n            preds = torch.argmax(output, dim=1)\n            matrix_size = [16, 29]\n            b=np.zeros((matrix_size[0],matrix_size[1]))\n            i = 0\n            while i < 16:\n                j = 0\n                while j < 29:\n                    b[i][j] = preds[16 * j + i]\n                    j += 1\n                i += 1\n\n            i = 0\n            while i < 16:\n              solution.append(reverse_tokenize(b[i]))\n              i+=1\n\n            non_zero_elements = trg != 0\n            non_zero_indices = non_zero_elements.nonzero(as_tuple=True)\n            non_pad_elements = non_zero_indices[0]\n\n\n            preds_non_pad = preds[non_pad_elements]\n            trg_non_pad = trg[non_pad_elements]\n            correct = preds_non_pad == trg_non_pad\n            \n\n            correct_sum = correct.sum().item()\n            non_pad_length = len(non_pad_elements)\n            accuracy = correct_sum / non_pad_length\n            epoch_acc += accuracy\n            index += 1\n          except StopIteration:\n            break  \n    loss_actual = epoch_loss / len(iterator)\n    accuracy_actual = epoch_acc / len(iterator)\n    return  loss_actual, accuracy_actual,solution\n\n\n\n\nCLIP = 1\nN_EPOCHS = 10\n\n\nbest_valid_loss = float('inf')\nepoch = 0\nwhile epoch < N_EPOCHS:\n    train_loss, train_acc = train(model, train_iter, optimizer, criterion, CLIP)\n    valid_loss, val_acc, solution = evaluate(model, valid_iter, criterion)\n    print(f'Epoch: {epoch+1:02}')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train accuracy: {train_acc*33:.3f}')\n    print(f'\\t Val. Loss: {valid_loss:.3f} | Val accuracy: {val_acc*33:.3f}')\n    epoch+=1\n\n\ntest_loss,test_accuracy,solution_test = evaluate(model, test_iter, criterion)\n\nprint(f'| Test Loss: {test_loss:.3f} | Test accuracy: {test_accuracy*33:7.3f} |')\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TEXT Prediction file\n","metadata":{}},{"cell_type":"code","source":"file_path = '/kaggle/input/marathi-dataset/mar/Predictions_vanilla.txt'\n\nwith open(file_path, 'w') as file:\n    file.write('English\\tMarathi_trans\\tVanilla_seq2seq_pred\\n')\n\n    # Write the data rows\n    for en, ma, pred in zip(test_en, test_ma, solution_test):\n        file.write(f'{en}\\t{ma}\\t{pred}\\n')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}