{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-17T17:55:22.616414Z","iopub.status.busy":"2024-05-17T17:55:22.615987Z","iopub.status.idle":"2024-05-17T17:55:26.416102Z","shell.execute_reply":"2024-05-17T17:55:26.414876Z","shell.execute_reply.started":"2024-05-17T17:55:22.616379Z"},"id":"Fl3oO7Gd7Yi7","outputId":"168a1e6c-a21f-42ee-e706-4358a15c0454","trusted":true},"outputs":[],"source":["import pandas as pd\n","import torch.optim as optim\n","import pandas as pd\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader\n","\n","import math\n","import time\n","import pandas as pd\n","import numpy as np\n","import random\n","from typing import Tuple\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import string\n","import random\n","from collections import Counter\n","import torch\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader\n"]},{"cell_type":"markdown","metadata":{"id":"5NDoNJFeN5TC"},"source":["# LOADING DATA FROM CSV FILES"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T17:55:26.419183Z","iopub.status.busy":"2024-05-17T17:55:26.418571Z","iopub.status.idle":"2024-05-17T17:55:26.425677Z","shell.execute_reply":"2024-05-17T17:55:26.424285Z","shell.execute_reply.started":"2024-05-17T17:55:26.419141Z"},"id":"8DUylqRTCSHJ","trusted":true},"outputs":[],"source":["def load_data(path):\n","    data = pd.read_csv(path, header=None, names=[\"en\", \"ma\", \"\"], skip_blank_lines=True)\n","    data = data.dropna(subset=['en', 'ma'])\n","    return data[['en', 'ma']]\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T17:55:26.426947Z","iopub.status.busy":"2024-05-17T17:55:26.426649Z","iopub.status.idle":"2024-05-17T17:55:26.647204Z","shell.execute_reply":"2024-05-17T17:55:26.645907Z","shell.execute_reply.started":"2024-05-17T17:55:26.426922Z"},"id":"drRm1kLlCdXy","trusted":true},"outputs":[],"source":["train = load_data(\"/kaggle/input/marathi-dataset/mar/mar_train.csv\")\n","val = load_data(\"/kaggle/input/marathi-dataset/mar/mar_valid.csv\")\n","test = load_data(\"/kaggle/input/marathi-dataset/mar/mar_test.csv\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"feSI-Xyl-DM2"},"source":["#TOKENIZE DATASET"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-17T17:55:26.651425Z","iopub.status.busy":"2024-05-17T17:55:26.650687Z","iopub.status.idle":"2024-05-17T17:55:26.724943Z","shell.execute_reply":"2024-05-17T17:55:26.723879Z","shell.execute_reply.started":"2024-05-17T17:55:26.651382Z"},"id":"g3t82UK3kgyF","outputId":"a5051068-7fad-4e6c-f9a8-2068603d5429","trusted":true},"outputs":[],"source":["def unique_tokenize(data):\n","    english_tokens = set(''.join(data['en']))\n","    marathi_tokens = set(''.join(data['ma']))\n","    return sorted(list(marathi_tokens)), sorted(list(english_tokens))\n","\n","\n","marathi_tokens , english_tokens = unique_tokenize(train)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-17T17:55:26.728311Z","iopub.status.busy":"2024-05-17T17:55:26.726563Z","iopub.status.idle":"2024-05-17T17:55:26.741927Z","shell.execute_reply":"2024-05-17T17:55:26.740682Z","shell.execute_reply.started":"2024-05-17T17:55:26.728268Z"},"id":"yeeJVQ2EjRqO","outputId":"2b5de9b9-6673-4f4c-e90e-a3722d9f8aed","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'ँ': 1, 'ं': 2, 'ः': 3, 'अ': 4, 'आ': 5, 'इ': 6, 'ई': 7, 'उ': 8, 'ऊ': 9, 'ऋ': 10, 'ऍ': 11, 'ए': 12, 'ऐ': 13, 'ऑ': 14, 'ओ': 15, 'औ': 16, 'क': 17, 'ख': 18, 'ग': 19, 'घ': 20, 'च': 21, 'छ': 22, 'ज': 23, 'झ': 24, 'ञ': 25, 'ट': 26, 'ठ': 27, 'ड': 28, 'ढ': 29, 'ण': 30, 'त': 31, 'थ': 32, 'द': 33, 'ध': 34, 'न': 35, 'प': 36, 'फ': 37, 'ब': 38, 'भ': 39, 'म': 40, 'य': 41, 'र': 42, 'ल': 43, 'ळ': 44, 'व': 45, 'श': 46, 'ष': 47, 'स': 48, 'ह': 49, '़': 50, 'ा': 51, 'ि': 52, 'ी': 53, 'ु': 54, 'ू': 55, 'ृ': 56, 'ॅ': 57, 'े': 58, 'ै': 59, 'ॉ': 60, 'ो': 61, 'ौ': 62, '्': 63, ' ': 0, ';': 65, '<unk>': 64, '.': 66}\n","{1: 'ँ', 2: 'ं', 3: 'ः', 4: 'अ', 5: 'आ', 6: 'इ', 7: 'ई', 8: 'उ', 9: 'ऊ', 10: 'ऋ', 11: 'ऍ', 12: 'ए', 13: 'ऐ', 14: 'ऑ', 15: 'ओ', 16: 'औ', 17: 'क', 18: 'ख', 19: 'ग', 20: 'घ', 21: 'च', 22: 'छ', 23: 'ज', 24: 'झ', 25: 'ञ', 26: 'ट', 27: 'ठ', 28: 'ड', 29: 'ढ', 30: 'ण', 31: 'त', 32: 'थ', 33: 'द', 34: 'ध', 35: 'न', 36: 'प', 37: 'फ', 38: 'ब', 39: 'भ', 40: 'म', 41: 'य', 42: 'र', 43: 'ल', 44: 'ळ', 45: 'व', 46: 'श', 47: 'ष', 48: 'स', 49: 'ह', 50: '़', 51: 'ा', 52: 'ि', 53: 'ी', 54: 'ु', 55: 'ू', 56: 'ृ', 57: 'ॅ', 58: 'े', 59: 'ै', 60: 'ॉ', 61: 'ो', 62: 'ौ', 63: '्', 65: ';', 0: '', 64: '<unk>', 66: '.'}\n","{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, ' ': 0, ';': 27, '.': 28}\n","mar: 67\n","eng: 29\n"]}],"source":["def tokenize_map(hindi_tokens, english_tokens):\n","    # Create token maps for English and Marathi\n","    english_token_map = {token: idx + 1 for idx, token in enumerate(english_tokens)}\n","    marathi_token_map = {token: idx + 1 for idx, token in enumerate(hindi_tokens)}\n","\n","    # Create reverse token map for Marathi\n","    reverse_marathi_token_map = {idx + 1: token for idx, token in enumerate(hindi_tokens)}\n","\n","    \n","    marathi_token_map[\" \"] = 0\n","    english_token_map[\" \"] = 0\n","    reverse_marathi_token_map[65] = ';'\n","    marathi_token_map[';'] = 65\n","    reverse_marathi_token_map[0] = ''\n","    marathi_token_map['<unk>'] = 64\n","    english_token_map[';'] = 27\n","    reverse_marathi_token_map[64] = '<unk>'\n","    reverse_marathi_token_map[66] = '.'\n","    english_token_map['.'] = 28\n","    marathi_token_map['.'] = 66\n","\n","\n","    return marathi_token_map, english_token_map, reverse_marathi_token_map\n","mar_token_map, eng_token_map,reverse_marathi_token_map = tokenize_map(marathi_tokens , english_tokens)\n","\n","print(mar_token_map)\n","print(reverse_marathi_token_map)\n","print(eng_token_map)\n","\n","print('mar:',len(mar_token_map))\n","print('eng:',len(eng_token_map))"]},{"cell_type":"markdown","metadata":{"id":"MOOqbrfKo-ks"},"source":["#MAXIMUM WORD LENGTH THAT ARE PRESENT IN THE DATASET\n","\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-17T17:55:26.745024Z","iopub.status.busy":"2024-05-17T17:55:26.744142Z","iopub.status.idle":"2024-05-17T17:55:26.763383Z","shell.execute_reply":"2024-05-17T17:55:26.762010Z","shell.execute_reply.started":"2024-05-17T17:55:26.744980Z"},"id":"1aMkFt73o-IW","outputId":"ab6a2141-8fb1-4167-985d-2693cd06d1a2","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["30 22\n"]}],"source":["\n","x = test['en'].values\n","y = test['ma'].values\n","\n","#Getting max length\n","max_eng_len = max([len(i) for i in x]) + 2\n","max_mar_len = max([len(i) for i in y]) + 2\n","print(max_eng_len,max_mar_len)"]},{"cell_type":"markdown","metadata":{"id":"i-ry8u0ousdZ"},"source":["# ONE HOT ENCODING/EMBEDDING OUR INPUT\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-17T17:55:26.765212Z","iopub.status.busy":"2024-05-17T17:55:26.764863Z","iopub.status.idle":"2024-05-17T17:55:37.310353Z","shell.execute_reply":"2024-05-17T17:55:37.309486Z","shell.execute_reply.started":"2024-05-17T17:55:26.765185Z"},"id":"fdffrLwysNo-","outputId":"0f61f3f7-1140-4de5-a547-90f9019972c6","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([51200, 30])\n","torch.Size([51200, 30])\n","torch.Size([51200, 30])\n","torch.Size([4096, 30])\n","torch.Size([4096, 30])\n","torch.Size([4096, 30])\n","torch.Size([4096, 30])\n","torch.Size([4096, 30])\n","torch.Size([4096, 30])\n"]}],"source":["\n","import torch\n","#unknown token present in validation set as 'r.(in marathi)'\n","unknown_token=64\n","def process(data):\n","    x,y = data['en'].values, data['ma'].values\n","    x = \";\" + x + \".\"\n","    y = \";\" + y + \".\"\n","    \n","    \n","    a = torch.zeros((len(x),max_eng_len),dtype=torch.int64)\n","    print(a.shape)\n","    \n","    b = torch.zeros((len(y),max_eng_len),dtype=torch.int64)\n","    \n","    data=[]\n","    for i,(xx,yy) in enumerate(zip(x,y)):\n","        for j,ch in enumerate(xx):\n","            a[i,j] = eng_token_map[ch]\n","\n","        #a[i,j+1:] = eng_token_map[\" \"]\n","        for j,ch in enumerate(yy):\n","            if ch in mar_token_map: \n","             b[i,j] = mar_token_map[ch]\n","            else:\n","              b[i,j]= unknown_token\n","\n","\n","    data = [(a[i], b[i]) for i in range(len(x))]\n","    print(a.shape)\n","    print(b.shape)\n","    return data\n","\n","train_process=process(train)\n","val_process=process(val)\n","test_process=process(test)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-17T17:55:37.312251Z","iopub.status.busy":"2024-05-17T17:55:37.311500Z","iopub.status.idle":"2024-05-17T17:55:37.317366Z","shell.execute_reply":"2024-05-17T17:55:37.316567Z","shell.execute_reply.started":"2024-05-17T17:55:37.312220Z"},"id":"w61HFE3F2aol","outputId":"3bec134a-d2b0-43cc-968b-8109d11a3919","trusted":true},"outputs":[],"source":["#Used later for reading the words\n","\n","from torch import tensor\n","def reverse_tokenize(data):\n","  data=[int(i) for i in data]\n","  predicted_seq = [reverse_marathi_token_map[idx] for idx in data]\n","  predicted_seq=''.join(predicted_seq)\n","  return predicted_seq\n","  "]},{"cell_type":"markdown","metadata":{"id":"9wDRhKZ4-LHs"},"source":["#DATA LOADER"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-17T17:55:37.319653Z","iopub.status.busy":"2024-05-17T17:55:37.318964Z","iopub.status.idle":"2024-05-17T17:55:37.334224Z","shell.execute_reply":"2024-05-17T17:55:37.332952Z","shell.execute_reply.started":"2024-05-17T17:55:37.319615Z"},"id":"p1SvwaaN-Lj9","outputId":"29549e79-d2c1-4489-912a-5b2d7b3f8856","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["100\n","8\n","8\n"]}],"source":["\n","import torch\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","BATCH_SIZE = 512\n","\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader\n","\n","\n","def dataL(P, BATCH_SIZE, shuffle_bool):\n","    iter = DataLoader(P, batch_size=BATCH_SIZE, shuffle=shuffle_bool)\n","\n","    return iter\n","\n","\n","\n","train_iter = dataL(train_process, BATCH_SIZE, False)\n","valid_iter = dataL(val_process, BATCH_SIZE, False)\n","test_iter = dataL(test_process, BATCH_SIZE, False)\n","\n","print(len(train_iter))\n","print(len(test_iter))\n","print(len(valid_iter))"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T17:55:37.340475Z","iopub.status.busy":"2024-05-17T17:55:37.340091Z","iopub.status.idle":"2024-05-17T17:55:37.349425Z","shell.execute_reply":"2024-05-17T17:55:37.348429Z","shell.execute_reply.started":"2024-05-17T17:55:37.340447Z"},"id":"547j1j6T2bEu","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import string\n","import random\n","from collections import Counter\n","# Set random seed for reproducibility\n","SEED = 1234\n","random.seed(SEED)\n","torch.manual_seed(SEED)\n","import os\n","#os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","#os.environ['TORCH_USE_CUDA_DSA'] = '1'\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T17:55:37.351673Z","iopub.status.busy":"2024-05-17T17:55:37.351006Z","iopub.status.idle":"2024-05-17T17:56:02.335025Z","shell.execute_reply":"2024-05-17T17:56:02.334075Z","shell.execute_reply.started":"2024-05-17T17:55:37.351633Z"},"id":"rnhhFib9Vrl_","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.6)\n","Collecting wandb\n","  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\n","Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (4.2.0)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\n","Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\n","Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.45.0)\n","Requirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\n","Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: wandb\n","  Attempting uninstall: wandb\n","    Found existing installation: wandb 0.16.6\n","    Uninstalling wandb-0.16.6:\n","      Successfully uninstalled wandb-0.16.6\n","Successfully installed wandb-0.17.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}],"source":["!pip install --upgrade wandb\n","!wandb login f6b40fc0bcc13c1d6117718b14ef1ddd1c68a700 #my API key for wandb login \n","import wandb"]},{"cell_type":"markdown","metadata":{"id":"P0GVbIJUNEds"},"source":["#ATTENTION MODEL"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-17T17:56:02.337292Z","iopub.status.busy":"2024-05-17T17:56:02.336637Z","iopub.status.idle":"2024-05-17T17:56:02.380354Z","shell.execute_reply":"2024-05-17T17:56:02.379089Z","shell.execute_reply.started":"2024-05-17T17:56:02.337248Z"},"id":"4HnpQyXnNBh0","outputId":"b6464bca-120d-40d2-be6f-12a87adb9169","trusted":true},"outputs":[],"source":["import random\n","from typing import Tuple\n","\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch import Tensor\n","\n","\n","class Encoder(nn.Module):\n","    \n","    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n","        super(Encoder, self).__init__()\n","        l = [input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout]\n","        self.input_dim = l[0]\n","        self.emb_dim = l[1]\n","        self.enc_hid_dim = l[2]\n","        self.dec_hid_dim = l[3]\n","        self.dropout = l[4]\n","\n","        self.embedding = nn.Embedding(l[0], l[1])\n","        bidir_bool = True   # Always True\n","        # GRU \n","        self.rnn = nn.GRU(l[1], l[2], bidirectional=bidir_bool)\n","        temp = enc_hid_dim * 2\n","        self.fc = nn.Linear(temp, l[3])\n","        self.dropout = nn.Dropout(l[4])\n","\n","    \n","\n","    \n","    def forward(self, src: Tensor) -> Tuple[Tensor]:\n","        # Transpose the source tensor\n","        src_transposed = src.transpose(0, 1)\n","\n","        # Embed the transposed source tensor with dropout\n","        embedded = self.dropout(self.embedding(src_transposed))\n","\n","        # Pass the embedded tensor through the RNN\n","        outputs, hidden = self.rnn(embedded)\n","\n","        # Concatenate and apply tanh activation to the last two layers of hidden states\n","        last_hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n","        hidden_combined = torch.tanh(self.fc(last_hidden))\n","\n","        return outputs, hidden_combined\n","\n","class Attention(nn.Module):\n","    \n","\n","    def __init__(self, enc_hid_dim: int, dec_hid_dim: int, attn_dim: int):\n","        super(Attention, self).__init__()\n","        hid_l = [enc_hid_dim, dec_hid_dim]\n","        self.enc_hid_dim = hid_l[0]\n","        self.dec_hid_dim = hid_l[1]\n","        enc_hid_dim_double = 2 * enc_hid_dim\n","        temp = enc_hid_dim_double + dec_hid_dim\n","        self.attn_in = temp\n","\n","        self.attn = nn.Linear(self.attn_in, attn_dim)\n","        \n","        \n","    def forward(self, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tensor:\n","        src_len = encoder_outputs.size(0)\n","        \n","        decoder_hidden_expanded = decoder_hidden.unsqueeze(1)\n","\n","        repeated_decoder_hidden = decoder_hidden_expanded.repeat(1, src_len, 1)\n","\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","\n","        concat_hidden_outputs = torch.cat((repeated_decoder_hidden, encoder_outputs), dim=2)\n","        linear_transform = self.attn(concat_hidden_outputs)\n","        energy = torch.tanh(linear_transform)\n","        attention = torch.sum(energy, dim=2)\n","\n","        attention_weights = F.softmax(attention, dim=1)\n","\n","        return attention_weights\n","    \n","    \n","class Decoder(nn.Module):\n","    \n","    def __init__(self, output_dim: int, emb_dim: int, enc_hid_dim: int,\n","                 dec_hid_dim: int, dropout: float, attention: nn.Module):\n","        super().__init__()\n","        cons_l = [output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention]\n","        self.output_dim = cons_l[0]\n","        self.emb_dim = cons_l[1]\n","        self.enc_hid_dim =cons_l[2] \n","        self.dec_hid_dim = cons_l[3]\n","        self.dropout_p = cons_l[4]\n","        self.attention = cons_l[5]\n","\n","        self.embedding = nn.Embedding(cons_l[0], cons_l[1])\n","        self.rnn_input_size = cons_l[2] * 2 + cons_l[1]\n","        self.rnn = nn.GRU(self.rnn_input_size, cons_l[3])\n","        self.out_input_size = self.attention.attn_in + cons_l[1]\n","        self.out = nn.Linear(self.out_input_size, cons_l[0])\n","        self.dropout = nn.Dropout(cons_l[4])\n","    \n","\n","\n","    def _weighted_encoder_rep(self, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tensor:\n","        # Calculate attention weights\n","        attention_weights = self.attention(decoder_hidden, encoder_outputs)\n","        attention_weights = attention_weights.unsqueeze(1)  # Add a dimension for batch multiplication\n","\n","        # Transpose encoder outputs for correct alignment\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","\n","        # Calculate weighted encoder representation\n","        weighted_encoder_rep = torch.bmm(attention_weights, encoder_outputs)\n","        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)  # Transpose back for output\n","\n","        return weighted_encoder_rep\n","    \n","    def forward(self, input: Tensor, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tuple[Tensor]:\n","        # Add a batch dimension to input\n","        input = input.unsqueeze(0)\n","\n","        # Transpose input tensor\n","        input = input.permute(1, 0)\n","\n","        # Apply embedding to the input tensor\n","        embedded_input = self.embedding(input)\n","\n","        # Apply dropout to the embedded input tensor\n","        embedded = self.dropout(embedded_input)\n","\n","        # Calculate the weighted encoder representation\n","        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden, encoder_outputs)\n","\n","        # Transpose embedded tensor for concatenation\n","        embedded = embedded.permute(1, 0, 2)\n","\n","        # Concatenate embedded tensor and weighted encoder representation\n","        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim=2)\n","\n","        # Pass through the GRU layer\n","        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n","\n","        # Squeeze the tensors to remove batch dimension\n","        embedded = embedded.squeeze(0)\n","        output = output.squeeze(0)\n","        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n","\n","        # Concatenate output, weighted encoder representation, and embedded tensor\n","        concatenated_output = torch.cat((output, weighted_encoder_rep, embedded), dim=1)\n","\n","        # Pass through the output layer\n","        final_output = self.out(concatenated_output)\n","\n","        return final_output, decoder_hidden.squeeze(0)\n","    \n","\n","\n","class Seq2Seq(nn.Module):\n","    def __init__(self,encoder: nn.Module,decoder: nn.Module,device: torch.device):\n","        super().__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","    def forward(self,src: Tensor,trg: Tensor,teacher_forcing_ratio: float = 0.5) -> Tensor:\n","\n","        batch_size = src.shape[0]\n","        max_len = trg.shape[1]\n","        trg_vocab_size = self.decoder.output_dim\n","        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n","        #ATTENTION HEAT MAP METHOD\n","        #attentions = torch.zeros(max_len, batch_size, src.shape[1]).to(self.device)\n","\n","        encoder_outputs, hidden = self.encoder(src)\n","\n","        # first input to the decoder is the <sos> token\n","        trg = trg.permute(1,0)\n","        output = trg[0,:]\n","        \n","        t = 1\n","        while t < max_len:\n","            output, hidden = self.decoder(output, hidden, encoder_outputs)\n","            outputs[t] = output\n","            if random.random() < teacher_forcing_ratio:\n","                output = trg[t]\n","            else:\n","                output = output.max(1)[1]\n","            t += 1\n","\n","        return outputs\n","\n","\n","\n","def init_weights(m: nn.Module):\n","    params = iter(m.named_parameters())\n","    try:\n","        while True:\n","            name, param = next(params)\n","            if 'weight' in name:\n","                nn.init.normal_(param.data, mean=0, std=0.01)\n","            else:\n","                nn.init.constant_(param.data, 0)\n","    except StopIteration:\n","        pass\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QP2vxY5EIul0"},"source":["# ATTENTION WANDB IMPLEMENTATION"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T17:58:38.055895Z","iopub.status.busy":"2024-05-17T17:58:38.055297Z"},"id":"KwA5nEgVItcQ","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"]},{"name":"stdout","output_type":"stream","text":["Create sweep with ID: soltfjhz\n","Sweep URL: https://wandb.ai/cs23m074yash/Deep_Learning_Seq2Seq_Transliteration_Assignment_3/sweeps/soltfjhz\n","VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))\n","<IPython.core.display.HTML object>\n","<IPython.core.display.HTML object>\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: n5y12tpl with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tattn_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbi_dir: True\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.7\n","\u001b[34m\u001b[1mwandb\u001b[0m: \temb_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlength_penalty: 0.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 10\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_fr: 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"]},{"data":{"text/html":["Tracking run with wandb version 0.17.0"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240517_175846-n5y12tpl</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/cs23m074yash/Deep_Learning_Seq2Seq_Transliteration_Assignment_3/runs/n5y12tpl' target=\"_blank\">treasured-sweep-1</a></strong> to <a href='https://wandb.ai/cs23m074yash/Deep_Learning_Seq2Seq_Transliteration_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs23m074yash/Deep_Learning_Seq2Seq_Transliteration_Assignment_3/sweeps/soltfjhz' target=\"_blank\">https://wandb.ai/cs23m074yash/Deep_Learning_Seq2Seq_Transliteration_Assignment_3/sweeps/soltfjhz</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/cs23m074yash/Deep_Learning_Seq2Seq_Transliteration_Assignment_3' target=\"_blank\">https://wandb.ai/cs23m074yash/Deep_Learning_Seq2Seq_Transliteration_Assignment_3</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View sweep at <a href='https://wandb.ai/cs23m074yash/Deep_Learning_Seq2Seq_Transliteration_Assignment_3/sweeps/soltfjhz' target=\"_blank\">https://wandb.ai/cs23m074yash/Deep_Learning_Seq2Seq_Transliteration_Assignment_3/sweeps/soltfjhz</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/cs23m074yash/Deep_Learning_Seq2Seq_Transliteration_Assignment_3/runs/n5y12tpl' target=\"_blank\">https://wandb.ai/cs23m074yash/Deep_Learning_Seq2Seq_Transliteration_Assignment_3/runs/n5y12tpl</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["import math\n","import time\n","\n","sweep_config = {\n","            \n","            'method': 'random',\n","            'metric': { 'goal': 'maximize','name': 'Accuracy'},\n","            'parameters': \n","                {\n","                    'num_epochs': {'values': [10]},\n","                    'cell_type': {'values': ['RNN', 'LSTM', 'GRU']},\n","                    'emb_dim': {'values': [128, 256, 512]},\n","                    'hidden_layer_dim': {'values': [128, 256, 512]},\n","                    'num_layers': {'values': [1, 2, 3]},\n","                    'dropout': {'values': [0.3, 0.5, 0.7]},\n","                    'attn_dim':{'values':[64,128,256]},\n","                    'optimizer' : {'values' : ['adam', 'rmsprop', 'adagrad']},\n","                    'learning_rate': {'values': [0.001, 0.005, 0.01, 0.1]},\n","                    'batch_size': {'values': [32, 64]},\n","                    'teacher_fr' : {'values': [0.3, 0.5, 0.7]},\n","                    'length_penalty' : {'values': [0.4, 0.5, 0.6]},\n","                    'bi_dir' : {'values': [True, False]},\n","                    'beam_width': {'values': [1, 2, 3, 4, 5]}\n","                }\n","            }\n","sweep_id = wandb.sweep(sweep_config, project=\"Deep_Learning_Seq2Seq_Transliteration_Assignment_3\")\n","def train_model(model, train_iter, optimizer, criterion, device, CLIP):\n","    model.train()\n","    train_epoch_loss = 0\n","    train_epoch_acc = 0\n","\n","    for _, (src, trg) in enumerate(train_iter):\n","        optimizer.zero_grad()\n","        src = src.to(device)\n","        trg = trg.to(device)\n","\n","        output = model(src, trg)\n","\n","        output = output[1:].view(-1, output.shape[-1])\n","        trg = trg.permute(1, 0)\n","        trg = torch.reshape(trg[1:], (-1,))\n","        loss = criterion(output, trg)\n","\n","        preds = torch.argmax(output, dim=1)\n","        non_pad_elements = (trg != 0).nonzero(as_tuple=True)[0]\n","        train_correct = preds[non_pad_elements] == trg[non_pad_elements]\n","        train_epoch_acc += train_correct.sum().item() / len(non_pad_elements)\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n","\n","        optimizer.step()\n","\n","        train_epoch_loss += loss.item()\n","\n","    train_epoch_loss /= len(train_iter)\n","    train_epoch_acc /= len(train_iter)\n","\n","    return train_epoch_loss, train_epoch_acc\n","\n","def evaluate_model(model, valid_iter, criterion, device):\n","    model.eval()\n","    val_epoch_loss = 0\n","    val_epoch_acc = 0\n","\n","    with torch.no_grad():\n","        for _, (src, trg) in enumerate(valid_iter):\n","            src, trg = src.to(device), trg.to(device)\n","\n","            output = model(src, trg, 0)\n","\n","            output = output[1:].view(-1, output.shape[-1])\n","            trg = trg.permute(1, 0)\n","            trg = torch.reshape(trg[1:], (-1,))\n","\n","            loss = criterion(output, trg)\n","\n","            val_epoch_loss += loss.item()\n","\n","            preds = torch.argmax(output, dim=1)\n","            non_pad_elements = (trg != 0).nonzero(as_tuple=True)[0]\n","            val_correct = preds[non_pad_elements] == trg[non_pad_elements]\n","            val_epoch_acc += val_correct.sum().item() / len(non_pad_elements)\n","\n","    val_epoch_loss /= len(valid_iter)\n","    val_epoch_acc /= len(valid_iter)\n","\n","    return val_epoch_loss, val_epoch_acc\n","\n","def run_training_loop(model, train_iter, valid_iter, optimizer, criterion, device, CLIP, N_EPOCHS):\n","    for epoch in range(N_EPOCHS):\n","        train_epoch_loss, train_epoch_acc = train_model(model, train_iter, optimizer, criterion, device, CLIP)\n","        val_epoch_loss, val_epoch_acc = evaluate_model(model, valid_iter, criterion, device)\n","\n","        print(f'Epoch: {epoch+1:02} | Train accuracy: {train_epoch_acc*100:.3f} | Val accuracy: {val_epoch_acc*100:.3f}')\n","        wandb.log({\"train_loss\": train_epoch_loss, \"train_accuracy\": train_epoch_acc, \"val_loss\": val_epoch_loss, \"val_accuracy\": val_epoch_acc})\n","\n","        if epoch == N_EPOCHS - 1:\n","            torch.cuda.empty_cache()\n","\n","\n","def sweep_train():\n","    # Default values for hyper-parameters we're going to sweep over\n","    config_defaults = {'emb_dim':256,'hidden_layer_dim':256,'attn_dim':128,'dropout':0.6,}\n","    # Initialize a new wandb run\n","    wandb.init(project='Deep_Learning_Seq2Seq_Transliteration_Assignment_3',config=config_defaults)\n","    wandb.run.name = (\n","        'Q2_c:' + str(wandb.config.cell_type) +\n","        '_e' + str(wandb.config.num_epochs) +\n","        '_es:' + str(wandb.config.emb_dim) +\n","        '_hs:' + str(wandb.config.hidden_layer_dim) +\n","        '_nle:' + str(wandb.config.num_layers) +\n","        '_nld:' + str(wandb.config.num_layers) +\n","        '_o:' + str(wandb.config.optimizer) +\n","        '_lr:' + str(wandb.config.learning_rate) +\n","        '_bs:' + str(wandb.config.batch_size) +\n","        '_tf:' + str(wandb.config.teacher_fr) +\n","        '_lp:' + str(wandb.config.length_penalty) +\n","        '_b:' + str(wandb.config.bi_dir) +\n","        '_bw:' + str(wandb.config.beam_width)\n","    ) \n","\n","    emb_dim = wandb.config.emb_dim\n","    hidden_layer_dim = wandb.config.hidden_layer_dim\n","    attn_dim = wandb.config.attn_dim\n","    dropout = wandb.config.dropout\n","\n","    token_map_list_length = [len(eng_token_map), len(mar_token_map)]  \n","    INPUT_DIM = token_map_list_length[0]\n","    OUTPUT_DIM = token_map_list_length[1]\n","    enc_param = [INPUT_DIM, emb_dim, hidden_layer_dim, hidden_layer_dim, dropout]\n","    enc = Encoder(enc_param[0], enc_param[1],enc_param[2],enc_param[3],enc_param[4])\n","    att_param = [hidden_layer_dim, hidden_layer_dim, attn_dim]\n","    attn = Attention(att_param[0], att_param[1], att_param[2])\n","    dec_param = [OUTPUT_DIM, emb_dim, hidden_layer_dim, hidden_layer_dim, dropout, attn]\n","    dec = Decoder(dec_param[0],dec_param[1],dec_param[2],dec_param[3],dec_param[4],dec_param[5])\n","\n","    model = Seq2Seq(enc, dec, device).to(device)\n","    model.apply(init_weights)\n","    opti = wandb.config.optimizer\n","    if opti == \"adam\":\n","        optimizer = optim.Adam(model.parameters())\n","    if opti == \"rmsprop\":\n","        optimizer = optim.RMSprop(model.parameters())\n","    if opti == \"adagrad\":\n","        optimizer = optim.Adagrad(model.parameters())\n","    else:\n","        optimizer = optim.Adam(model.parameters())  \n","\n","    criterion = nn.CrossEntropyLoss()\n","\n","    model.train()\n","    train_epoch_loss = 0\n","    val_epoch_loss,val_epoch_acc = 0,0\n","    train_epoch_acc = 0\n","    \n","\n","    N_EPOCHS = wandb.config.num_epochs\n","    CLIP = 1\n","\n","\n","    # Example usage:\n","    run_training_loop(model, train_iter, valid_iter, optimizer, criterion, device, CLIP, N_EPOCHS)\n","\n","#RUNNING THE SWEEP\n","wandb.agent(sweep_id, function=sweep_train, count=120)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T17:56:51.405563Z","iopub.status.busy":"2024-05-17T17:56:51.404763Z","iopub.status.idle":"2024-05-17T17:57:52.108621Z","shell.execute_reply":"2024-05-17T17:57:52.106612Z","shell.execute_reply.started":"2024-05-17T17:56:51.405498Z"},"trusted":true},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m N_EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     40\u001b[0m CLIP \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 41\u001b[0m \u001b[43mrun_training_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[13], line 92\u001b[0m, in \u001b[0;36mrun_training_loop\u001b[0;34m(model, train_iter, valid_iter, optimizer, criterion, device, CLIP, N_EPOCHS)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_training_loop\u001b[39m(model, train_iter, valid_iter, optimizer, criterion, device, CLIP, N_EPOCHS):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS):\n\u001b[0;32m---> 92\u001b[0m         train_epoch_loss, train_epoch_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m         val_epoch_loss, val_epoch_acc \u001b[38;5;241m=\u001b[39m evaluate_model(model, valid_iter, criterion, device)\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_epoch_acc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_epoch_acc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","Cell \u001b[0;32mIn[13], line 48\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_iter, optimizer, criterion, device, CLIP)\u001b[0m\n\u001b[1;32m     46\u001b[0m train_correct \u001b[38;5;241m=\u001b[39m preds[non_pad_elements] \u001b[38;5;241m==\u001b[39m trg[non_pad_elements]\n\u001b[1;32m     47\u001b[0m train_epoch_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_correct\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(non_pad_elements)\n\u001b[0;32m---> 48\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), CLIP)\n\u001b[1;32m     52\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["config_defaults = {'emb_dim':256,'hidden_layer_dim':256,'attn_dim':128,'dropout':0.6,}\n","    \n","emb_dim = 256\n","hidden_layer_dim = 256\n","attn_dim = 128\n","dropout = 0.6\n","\n","token_map_list_length = [len(eng_token_map), len(mar_token_map)]  \n","INPUT_DIM = token_map_list_length[0]\n","OUTPUT_DIM = token_map_list_length[1]\n","enc_param = [INPUT_DIM, emb_dim, hidden_layer_dim, hidden_layer_dim, dropout]\n","enc = Encoder(enc_param[0], enc_param[1],enc_param[2],enc_param[3],enc_param[4])\n","att_param = [hidden_layer_dim, hidden_layer_dim, attn_dim]\n","attn = Attention(att_param[0], att_param[1], att_param[2])\n","dec_param = [OUTPUT_DIM, emb_dim, hidden_layer_dim, hidden_layer_dim, dropout, attn]\n","dec = Decoder(dec_param[0],dec_param[1],dec_param[2],dec_param[3],dec_param[4],dec_param[5])\n","\n","model = Seq2Seq(enc, dec, device).to(device)\n","model.apply(init_weights)\n","opti = \"adam\"\n","if opti == \"adam\":\n","    optimizer = optim.Adam(model.parameters())\n","if opti == \"rmsprop\":\n","    optimizer = optim.RMSprop(model.parameters())\n","if opti == \"adagrad\":\n","    optimizer = optim.Adagrad(model.parameters())\n","else:\n","    optimizer = optim.Adam(model.parameters())  \n","\n","criterion = nn.CrossEntropyLoss()\n","\n","model.train()\n","\n","train_epoch_loss = 0\n","train_epoch_acc = 0\n","val_epoch_loss = 0\n","val_epoch_acc = 0\n","\n","N_EPOCHS = 10\n","CLIP = 1\n","run_training_loop(model, train_iter, valid_iter, optimizer, criterion, device, CLIP, N_EPOCHS)"]},{"cell_type":"markdown","metadata":{},"source":["Vanilla Seq2Seq\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:57:52.110452Z","iopub.status.idle":"2024-05-17T17:57:52.111194Z","shell.execute_reply":"2024-05-17T17:57:52.110908Z","shell.execute_reply.started":"2024-05-17T17:57:52.110882Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader\n","\n","import math\n","import time\n","import pandas as pd\n","import numpy as np\n","import random\n","from typing import Tuple\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import string\n","import random\n","from collections import Counter\n","import torch\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader\n"]},{"cell_type":"markdown","metadata":{},"source":["# Data Load"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:57:52.113184Z","iopub.status.idle":"2024-05-17T17:57:52.114289Z","shell.execute_reply":"2024-05-17T17:57:52.114004Z","shell.execute_reply.started":"2024-05-17T17:57:52.113975Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","def load_data(path):\n","    data = pd.read_csv(path, header=None, names=[\"en\", \"ma\", \"\"], skip_blank_lines=True)\n","    data = data.dropna(subset=['en', 'ma'])\n","    return data[['en', 'ma']]\n","\n","# Define file paths\n","file_paths = [\n","    \"/kaggle/input/marathi-dataset/mar/mar_train.csv\",\n","    \"/kaggle/input/marathi-dataset/mar/mar_valid.csv\",\n","    \"/kaggle/input/marathi-dataset/mar/mar_test.csv\"\n","]\n","\n","# Load data for train, val, and test\n","train, val, test = [load_data(path) for path in file_paths]\n","\n","# Display test data and lengths\n","# print(test)\n","test_en = test['en'].tolist()\n","test_ma = test['ma'].tolist()\n","# print(test_en)\n","# print(test_ma)\n","print(len(train))\n","print(len(test))\n","print(len(val))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:57:52.116535Z","iopub.status.idle":"2024-05-17T17:57:52.117103Z","shell.execute_reply":"2024-05-17T17:57:52.116845Z","shell.execute_reply.started":"2024-05-17T17:57:52.116820Z"},"trusted":true},"outputs":[],"source":["def unique_tokenize(data):\n","    english_tokens = set(''.join(data['en']))\n","    marathi_tokens = set(''.join(data['ma']))\n","    return sorted(list(marathi_tokens)), sorted(list(english_tokens))\n","marathi_tokens , english_tokens = unique_tokenize(train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:57:52.119409Z","iopub.status.idle":"2024-05-17T17:57:52.119995Z","shell.execute_reply":"2024-05-17T17:57:52.119726Z","shell.execute_reply.started":"2024-05-17T17:57:52.119703Z"},"trusted":true},"outputs":[],"source":["#REWRITE AGAIN\n","def tokenize_map(hindi_tokens , english_tokens):\n","    english_token_map = dict([(ch,i+1) for i,ch in enumerate(english_tokens)])\n","    marathi_token_map = dict([(ch,i+1) for i,ch in enumerate(marathi_tokens)])\n","    reverse_marathi_token_map = dict([(i+1,ch) for i,ch in enumerate(marathi_tokens)])\n","    #Adding blank space\n","\n","    marathi_token_map[\" \"] = 0\n","    english_token_map[\" \"] = 0\n","    #addin BOS and EOS token \n","    marathi_token_map[';']=65\n","    marathi_token_map['.']=66\n","    english_token_map[';']=27\n","    english_token_map['.']=28\n","    marathi_token_map['<unk>']=64\n","    \n","    reverse_marathi_token_map[64]='<unk>'\n","    reverse_marathi_token_map[65]=';'\n","    reverse_marathi_token_map[66]='.'\n","    reverse_marathi_token_map[0]=''\n","\n","    return marathi_token_map, english_token_map,reverse_marathi_token_map\n","\n","mar_token_map, eng_token_map,reverse_marathi_token_map = tokenize_map(marathi_tokens , english_tokens)\n","print(mar_token_map)\n","print(reverse_marathi_token_map)\n","print(eng_token_map)\n","\n","print('mar:',len(mar_token_map))\n","print('eng:',len(eng_token_map))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:57:52.122199Z","iopub.status.idle":"2024-05-17T17:57:52.122770Z","shell.execute_reply":"2024-05-17T17:57:52.122501Z","shell.execute_reply.started":"2024-05-17T17:57:52.122478Z"},"trusted":true},"outputs":[],"source":["\n","x = test['en'].values\n","y = test['ma'].values\n","\n","max_eng_len = 2+max([len(i) for i in x]) \n","max_mar_len = 2+max([len(i) for i in y])\n","print(max_eng_len,max_mar_len)"]},{"cell_type":"markdown","metadata":{},"source":["# One hot Encode"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:57:52.125022Z","iopub.status.idle":"2024-05-17T17:57:52.125608Z","shell.execute_reply":"2024-05-17T17:57:52.125321Z","shell.execute_reply.started":"2024-05-17T17:57:52.125299Z"},"trusted":true},"outputs":[],"source":["\n","import torch\n","unknown_token=64\n","def process(data):\n","    x, y = data['en'].values, data['ma'].values\n","\n","    # Add start and end tokens to sentences\n","    x = [';' + sentence + '.' for sentence in x]\n","    y = [';' + sentence + '.' for sentence in y]\n","    a = torch.zeros((len(x), max_eng_len), dtype=torch.int64)\n","    b = torch.zeros((len(y), max_eng_len), dtype=torch.int64)\n","\n","    # Process data\n","    for i, (xx, yy) in enumerate(zip(x, y)):\n","        for j, ch in enumerate(xx):\n","            a[i, j] = eng_token_map.get(ch, unknown_token)\n","\n","        for j, ch in enumerate(yy):\n","            b[i, j] = mar_token_map.get(ch, unknown_token)\n","\n","\n","    # Create list of tuples (input tensor, target tensor)\n","    data = [(a[i], b[i]) for i in range(len(x))]\n","\n","    return data\n","\n","\n","train_process=process(train)\n","val_process=process(val)\n","test_process=process(test)#print(train_process.shape)\n","print('\\n')\n","print('num of rows:',len(train_process))\n","print('num of columns:',len(train_process[0]))\n","print(train_process[0][1])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:57:52.127638Z","iopub.status.idle":"2024-05-17T17:57:52.129019Z","shell.execute_reply":"2024-05-17T17:57:52.128727Z","shell.execute_reply.started":"2024-05-17T17:57:52.128700Z"},"trusted":true},"outputs":[],"source":["\n","def reverse_tokenize(data):\n","    data = list(map(int, data))\n","    predicted_seq = ''.join(map(lambda idx: reverse_marathi_token_map[idx], data))\n","    return predicted_seq"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:57:52.130587Z","iopub.status.idle":"2024-05-17T17:57:52.131750Z","shell.execute_reply":"2024-05-17T17:57:52.131451Z","shell.execute_reply.started":"2024-05-17T17:57:52.131424Z"},"trusted":true},"outputs":[],"source":["\n","BATCH_SIZE = 16\n","\n","\n","def dataL(P, BATCH_SIZE, shuffle_bool):\n","    iter = DataLoader(P, batch_size=BATCH_SIZE, shuffle=shuffle_bool)\n","\n","    return iter\n","\n","\n","\n","train_iter = dataL(train_process, BATCH_SIZE, False)\n","valid_iter = dataL(val_process, BATCH_SIZE, False)\n","test_iter = dataL(test_process, BATCH_SIZE, False)\n","\n","print(len(train_iter))\n","print(len(test_iter))\n","print(len(valid_iter))"]},{"cell_type":"markdown","metadata":{},"source":["# Wandb login"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:57:52.133204Z","iopub.status.idle":"2024-05-17T17:57:52.133860Z","shell.execute_reply":"2024-05-17T17:57:52.133664Z","shell.execute_reply.started":"2024-05-17T17:57:52.133646Z"},"trusted":true},"outputs":[],"source":["\n","!wandb login f6b40fc0bcc13c1d6117718b14ef1ddd1c68a700 #my API key for wandb login \n","import wandb"]},{"cell_type":"markdown","metadata":{},"source":["# Vanilla encoder and decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:57:52.135972Z","iopub.status.idle":"2024-05-17T17:57:52.136831Z","shell.execute_reply":"2024-05-17T17:57:52.136627Z","shell.execute_reply.started":"2024-05-17T17:57:52.136608Z"},"trusted":true},"outputs":[],"source":["# Define the vocabulary of English and Devanagari characters\n","\n","# Define the maximum sequence lengths for input and output sequences\n","MAX_LEN_EN = 30\n","MAX_LEN_MA = 30\n","\n","# Define the start and end of sequence tokens\n","BOS_token = 28\n","EOS_token = 29\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self, input_size, embedding_size, hidden_size, num_layers=1 , cell_type=\"lstm\", p=0.5):\n","        super().__init__()\n","        eL = [input_size, embedding_size, hidden_size, num_layers , cell_type, p]\n","        self.embedding = nn.Embedding(eL[0],eL[1])\n","        self.hidden_size = eL[2]\n","        self.num_layers = eL[3]\n","        self.cell_type = eL[4]\n","        self.dropout=nn.Dropout(eL[5])\n","        prob = p\n","        if cell_type.upper() == \"GRU\":\n","            self.rnn = nn.GRU(input_size, hidden_size, num_layers,dropout=prob)\n","        if cell_type.upper() == \"LSTM\":\n","            self.rnn = nn.LSTM(input_size, hidden_size, num_layers,dropout=prob)\n","        if cell_type.upper() != \"RNN\":\n","            self.rnn = nn.GRU(input_size, hidden_size, num_layers,dropout=prob)\n","        else:\n","            self.rnn = nn.RNN(input_size, hidden_size, num_layers,dropout=prob)\n","    def forward(self, x):\n","        \n","        x = x.permute(1,0)\n","        pre_embedding = self.embedding(x)\n","        embedding=self.dropout(pre_embedding)\n","        if self.cell_type != \"lstm\":\n","            hidden, cell = self.rnn(embedding)\n","        elif self.cell_type == \"lstm\":\n","            outputs,(hidden,cell)=self.rnn(embedding)\n","        return hidden, cell\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers=1, cell_type=\"lstm\",p=0.5):\n","        super().__init__()\n","        dL = [input_size, embedding_size, hidden_size, output_size, num_layers, cell_type,p]\n","        self.embedding=nn.Embedding(dL[0], dL[1])\n","        self.hidden_size = dL[2]\n","        self.cell_type=dL[5]\n","        self.num_layers = dL[4]\n","        self.dropout=nn.Dropout(dL[6])\n","\n","        self.output_size = dL[3]\n","        if cell_type.upper() == \"GRU\":\n","            self.rnn = nn.GRU(output_size, hidden_size, num_layers)\n","        if cell_type.lower() != \"lstm\":\n","            self.rnn = nn.RNN(output_size, hidden_size, num_layers)\n","        if cell_type.upper() == \"RNN\":\n","            self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers,dropout=p)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x, hidden,cell):\n","       \n","       x = x.unsqueeze(0)\n","       pre_embeddingD = self.embedding(x) \n","       embedding= self.dropout(pre_embeddingD)\n","       \n","       if self.cell_type!=\"lstm\":\n","         outputs, hidden = self.rnn(embedding, hidden)\n","       else:\n","         outputs,(hidden,cell) = self.rnn(embedding, (hidden,cell))\n","       \n","\n","       predictions=self.fc(outputs)\n","       \n","       tempD = 0\n","       predictions = predictions.squeeze(tempD)\n","       \n","       if self.cell_type != \"lstm\":\n","         return predictions, hidden\n","       else:\n","         return predictions, hidden, cell\n","\n","class Seq2Seq(nn.Module):\n","    def __init__(self,encoder,decoder,cell_type=\"lstm\"):\n","        sL = [encoder,decoder,cell_type]\n","        super().__init__()\n","        self.encoder = sL[0]\n","        self.decoder = sL[1]\n","        self.cell_type = sL[2]\n","\n","    def forward(self, source, target, teacher_force_ratio=0.5):\n","        sFL = [source, target, teacher_force_ratio]\n","        batch_size = sFL[0].shape[0]\n","        \n","        target_len = sFL[1].shape[1]\n","        target_vocab_size = self.decoder.output_size\n","\n","        outputs = torch.zeros(target_len,batch_size,target_vocab_size).to(device)\n","        \n","        cell, hidden = self.encoder(sFL[0])\n","        \n","        target = sFL[1].permute(1,0)\n","        x = target[0,:]\n","        t = 1\n","        while t < target_len:\n","          if self.cell_type!=\"lstm\":\n","            output, hidden = self.decoder(x,hidden,cell)\n","          else:\n","           output,hidden,cell =self.decoder(x,hidden,cell)\n","\n","          outputs[t] = output\n","          \n","          best_guess = output.argmax(1)\n","          if random.random() >= teacher_force_ratio:\n","            x = best_guess  \n","          else:\n","            x = target[t]\n","          t += 1\n","        return outputs\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:57:52.138170Z","iopub.status.idle":"2024-05-17T17:57:52.138698Z","shell.execute_reply":"2024-05-17T17:57:52.138493Z","shell.execute_reply.started":"2024-05-17T17:57:52.138476Z"},"trusted":true},"outputs":[],"source":["\n","device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","num_epochs=10\n","learning_rate=0.001\n","len_token_map = [len(eng_token_map), len(mar_token_map)]\n","input_size_encoder = len_token_map[0]\n","input_size_decoder= len_token_map[1]\n","output_size= len_token_map[1]\n","max_size_E_D = [29, 67]\n","encoder_embedding_size = max_size_E_D[0]\n","decoder_embedding_size = max_size_E_D[1]\n","\n","cell_type=\"gru\"\n","hidden_size=256\n","num_layers=4\n","enc_dropout=0.2\n","dec_dropout=0.2\n","train_L = [input_size_encoder, encoder_embedding_size, hidden_size, num_layers , cell_type]\n","encoder_net=Encoder(len_token_map[0],max_size_E_D[0],train_L[2],train_L[3],train_L[4],p=enc_dropout).to(device)\n","decoder_net=Decoder(len_token_map[1],max_size_E_D[1],train_L[2],len_token_map[1],train_L[3],train_L[4],p=dec_dropout).to(device)\n","model_L = [encoder_net,decoder_net,cell_type]\n","model = Seq2Seq(model_L[0],model_L[1],model_L[2]).to(device)\n","criterion = nn.CrossEntropyLoss()\n","\n","\n","optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n","def train(model: nn.Module, iterator: torch.utils.data.DataLoader, optimizer: optim.Optimizer, criterion: nn.Module, clip: float):\n","\n","\n","    epoch_loss, epoch_acc = 0, 0\n","    model.train()\n","\n","\n","    iterator_iter = iter(iterator)\n","    index = 0\n","    while True:\n","        try:\n","            src, trg = next(iterator_iter)\n","            optimizer.zero_grad()\n","            src = src.to(device)\n","            trg = trg.to(device)\n","            output = model(src, trg)\n","\n","            output = output[1:].view(-1, output.shape[-1])\n","            trg = trg.permute(1,0)\n","            trg = torch.reshape(trg[1:], (-1,))\n","            loss = criterion(output, trg)\n","\n","            # calculate accuracy\n","            preds = torch.argmax(output, dim=1)\n","\n","            non_zero_elements = (trg != 0)\n","            non_zero_indices = non_zero_elements.nonzero(as_tuple=True)\n","            non_pad_elements = non_zero_indices[0]\n","\n","\n","            preds_non_pad = preds[non_pad_elements]\n","            trg_non_pad = trg[non_pad_elements]\n","            correct = preds_non_pad == trg_non_pad\n","\n","            correct_sum = correct.sum().item()\n","            non_pad_length = len(non_pad_elements)\n","            accuracy = correct_sum / non_pad_length\n","            epoch_acc = epoch_acc + accuracy\n","\n","            loss.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","\n","            optimizer.step()\n","\n","            epoch_loss = epoch_loss + loss.item()\n","            index += 1\n","        except StopIteration:\n","            break\n","    loss_actual = epoch_loss / len(iterator)\n","    accuracy_actual = epoch_acc / len(iterator)\n","    return  loss_actual, accuracy_actual\n","\n","\n","def evaluate(model: nn.Module,iterator: torch.utils.data.DataLoader,criterion: nn.Module):\n","    epoch_loss, epoch_acc = 0, 0\n","\n","    solution=[]\n","    model.eval()\n","    \n","    with torch.no_grad():\n","        iterator_iter = iter(iterator)\n","        \n","        index = 0\n","        while True:\n","          try:\n","            src, trg = next(iterator_iter)\n","            src = src.to(device)\n","            trg = trg.to(device)\n","\n","            output = model(src, trg, 0) #turn off teacher forcing\n","            output = output[1:].view(-1, output.shape[-1])\n","            \n","            trg = trg.permute(1,0)\n","            trg = torch.reshape(trg[1:], (-1,))\n","\n","            loss = criterion(output, trg)\n","\n","            epoch_loss = epoch_loss + loss.item()\n","            # calculate accuracy\n","            preds = torch.argmax(output, dim=1)\n","            matrix_size = [16, 29]\n","            b=np.zeros((matrix_size[0],matrix_size[1]))\n","            i = 0\n","            while i < 16:\n","                j = 0\n","                while j < 29:\n","                    b[i][j] = preds[16 * j + i]\n","                    j += 1\n","                i += 1\n","\n","            i = 0\n","            while i < 16:\n","              solution.append(reverse_tokenize(b[i]))\n","              i+=1\n","\n","            non_zero_elements = trg != 0\n","            non_zero_indices = non_zero_elements.nonzero(as_tuple=True)\n","            non_pad_elements = non_zero_indices[0]\n","\n","\n","            preds_non_pad = preds[non_pad_elements]\n","            trg_non_pad = trg[non_pad_elements]\n","            correct = preds_non_pad == trg_non_pad\n","            \n","\n","            correct_sum = correct.sum().item()\n","            non_pad_length = len(non_pad_elements)\n","            accuracy = correct_sum / non_pad_length\n","            epoch_acc += accuracy\n","            index += 1\n","          except StopIteration:\n","            break  \n","    loss_actual = epoch_loss / len(iterator)\n","    accuracy_actual = epoch_acc / len(iterator)\n","    return  loss_actual, accuracy_actual,solution\n","\n","\n","\n","\n","CLIP = 1\n","N_EPOCHS = 10\n","\n","\n","best_valid_loss = float('inf')\n","epoch = 0\n","while epoch < N_EPOCHS:\n","    train_loss, train_acc = train(model, train_iter, optimizer, criterion, CLIP)\n","    valid_loss, val_acc, solution = evaluate(model, valid_iter, criterion)\n","    print(f'Epoch: {epoch+1:02}')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train accuracy: {train_acc*33:.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} | Val accuracy: {val_acc*33:.3f}')\n","    epoch+=1\n","\n","\n","test_loss,test_accuracy,solution_test = evaluate(model, test_iter, criterion)\n","\n","print(f'| Test Loss: {test_loss:.3f} | Test accuracy: {test_accuracy*33:7.3f} |')\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# TEXT Prediction file\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-17T17:57:52.139984Z","iopub.status.idle":"2024-05-17T17:57:52.140662Z","shell.execute_reply":"2024-05-17T17:57:52.140412Z","shell.execute_reply.started":"2024-05-17T17:57:52.140394Z"},"trusted":true},"outputs":[],"source":["file_path = '/kaggle/input/marathi-dataset/mar/test_dataset_predictions.txt'\n","\n","with open(file_path, 'w') as file:\n","    file.write('Correct word\\tPredicted word\\n')\n","\n","    # Write the data rows\n","    for en, ma, pred in zip(test_en, test_ma, solution_test):\n","        file.write(f'{ma}\\t{pred}\\n')\n"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5017458,"sourceId":8426411,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0029bd3ca1c94350a4ed311349e040bf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"009d917192ac45f8b9a5cb3e88b3be67":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09dc089336be4b8a9fb06ebd4e27b92e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c4c5300439a47fca5d0dc2bbb00c7c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e20596ffc514c82b89c308c78e60097":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_009d917192ac45f8b9a5cb3e88b3be67","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_99445299e74d4e7db3951662a977c21a","value":0.11012801204819277}},"2de8fbf1431348c0b375c2bea37bc58a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f6b8df6ebb44ab1b38f5aebd30225d5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fb0c0dc7f564ad5849f01201a2349dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_fafea03cb3874a209280303fd0ac44e5","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_66c1662f065e4171803abdb4210d4df0","value":1}},"3726b5cd9ab240f198b0abf2bd0ee999":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cb5d0076b4e4ce8bf8943a44895bdc6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_ee966b1121cd45c28893b2cc3a0f36ef","IPY_MODEL_1e20596ffc514c82b89c308c78e60097"],"layout":"IPY_MODEL_09dc089336be4b8a9fb06ebd4e27b92e"}},"3f8878c3e01f4dfe954752b6a726974c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5cffdb42030b4aaaa4adf32ed67a4bd0","placeholder":"​","style":"IPY_MODEL_d490c128677e486981c7ed932e7a8e4a","value":"Waiting for wandb.init()...\r"}},"3fa2e79d8bf645fd925b2ca74b33ace1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4149a4c78e664d71a256e6deaab7716d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_3f8878c3e01f4dfe954752b6a726974c","IPY_MODEL_2fb0c0dc7f564ad5849f01201a2349dc"],"layout":"IPY_MODEL_3726b5cd9ab240f198b0abf2bd0ee999"}},"4562bbd5ab014f7084a5b74100662d6c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4db95218bd8c45fd8638e73b17655541":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e3528158aaf4a6687bdeb93fd7164d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5cffdb42030b4aaaa4adf32ed67a4bd0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62351b579cad492bafe79bec945e8327":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fa2e79d8bf645fd925b2ca74b33ace1","placeholder":"​","style":"IPY_MODEL_2de8fbf1431348c0b375c2bea37bc58a","value":"0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\r"}},"62cdf5406f4f4ae0bfef029acc8b25ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_0029bd3ca1c94350a4ed311349e040bf","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_99b20e257eb94f7c9ea138b138b8b92a","value":1}},"66c1662f065e4171803abdb4210d4df0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"770c3cb14d53452296ae96b6d12acb2b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99445299e74d4e7db3951662a977c21a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"99b20e257eb94f7c9ea138b138b8b92a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c269d71a857e408aa28d91b2efd35eff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4204db901f44e57a468b949be7a9cc5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0e13e633f6c46749d354d038df3dfff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_4562bbd5ab014f7084a5b74100662d6c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4e3528158aaf4a6687bdeb93fd7164d3","value":1}},"d2c4d6f23f594ef3b9bb9514eb986abb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_62351b579cad492bafe79bec945e8327","IPY_MODEL_62cdf5406f4f4ae0bfef029acc8b25ac"],"layout":"IPY_MODEL_770c3cb14d53452296ae96b6d12acb2b"}},"d490c128677e486981c7ed932e7a8e4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8e252f6d5d141bba53ec2757bbb498f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_ed6fb297d7cb4188953724e8fd678c56","IPY_MODEL_d0e13e633f6c46749d354d038df3dfff"],"layout":"IPY_MODEL_2f6b8df6ebb44ab1b38f5aebd30225d5"}},"ed6fb297d7cb4188953724e8fd678c56":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4db95218bd8c45fd8638e73b17655541","placeholder":"​","style":"IPY_MODEL_c4204db901f44e57a468b949be7a9cc5","value":"0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\r"}},"ee966b1121cd45c28893b2cc3a0f36ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c269d71a857e408aa28d91b2efd35eff","placeholder":"​","style":"IPY_MODEL_1c4c5300439a47fca5d0dc2bbb00c7c5","value":"0.001 MB of 0.010 MB uploaded (0.000 MB deduped)\r"}},"fafea03cb3874a209280303fd0ac44e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":4}
